
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{amsmath}

%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Created=Thu Feb 13 11:08:58 2003}
%TCIDATA{LastRevised=Thu Feb 13 15:52:02 2003}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{CSTFile=LaTeX article (bright).cst}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{tcilatex}

\begin{document}


If one looks at the vast variety of approaches to wavelets, it appears to be
a vague concept with matrix transforms and functions for the use in
compression. This is only the tip of the iceberg. In reality, wavelets offer
significant advantages in the field of numerical analysis. Some methods are
good for showing theoretical proofs. Other methods are good for computing
out solutions for frequency and filtering problems.

Goals for this introductory document into wavelets is to visit some
approaches to defining what a wavelet is, some basic theorems, and practical
examples. The wavelet definitions approaches a wavelet from both the matrix
and convolution methods. Most of the basic theorems presented here are based
on the convolution approach to wavelets. It is not to say that the matrix
approach is not valid, rather it is simply more complicated than this
introduction should be. Lastly the practical examples are based on the Haar
Wavelet applied to 1-D and 2-D sources. Such examples are also practical for
Daubechies wavelet transform as well.

Wavelet Transform Definitions:

As stated, the wavelet transform can be viewed from both a matrix
representation which allows many linear algebra and abstract algebra methods
to prove theorems about wavelets. Another form is a convolution type of
operation. The compact form used by James Walker is simply a special case of
the convolution method, and thus James Walker's principles apply to the
convolution methods.

In the matrix form, a wavelet matrix is defined as a generalization of a
square orthogonal or unitary matrices which are a subset of a larger class
of rectrangular matrices. The matrices contain information to define an
associate wavelet system. Such matrices are defined in terms of their rank
and genus.

Furthermore, Alfred Haar himself pointed out in Zur Theorie der orthogonalen
Funktionensystem Math Annual qualities of a Haar Wavelet Matrix. In these
cases, the Haar Wavelet Matrix has a genus of one. Such matrices have a one
to one mapping to a general wavelet matrix. Such Haar matrices are called
the characteristic Haar Matrix. Also a rank 2 Haar Matrix suffices to show
all geometric considerations.

The thing that makes the matrix representation difficult is the use of the
Laurent series. Also, computationally, any matrix operation is inherently $%
O^{2}$ a operation which is not desired in compuationaly intense programs.
Convolution operatated versions may offer an improvement.

James S. Walker identified eight properties of wavelets in convolution,
which some of them hold in general. These properties covered the following
areas: small fluctuations, compaction of energy, scaling squared, linear
fluctuations, quadratic fluctations, polynomial fluctuations. These
properties allow frequency components to be analyized in a wavelets space as
opposed to pure frequency space like Fourier and Laplace transforms allow.

\section{Wavelets via Convolution}

Wavelets can be defined in a matrix form as ($A|D_{1}|D_{2}|...|D_{n}$)
where n is the resolution of the wavelet, A is the average component, and is
the difference component. This implies that there is the ability to
transform a signal into this form and to recover it as well.

Many such as Walker, use a form that eliminates half of the values. This
method makes the total number values in the wavelet transform and the
original the same. However, there are other means. One method (an
over-qualified method) ensures that each component of the wavelet matrix is
of the same length as the orignal. It is hoped that properties of both
methods can reveal useful mathematical properties.

Another useful property of wavelets via convolution is the simplisity of the
operation The convolution is $O\left( n\log n\right) $in complexity. Where
as a matrix multiply is inherently $O(n^{2\,})$ . Obviously few operations
lead to less complexity.

\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `basename #1 .tif`.png}

\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{Brief Article}
\author{The Author}
\begin{document}
\maketitle




 \end{document}
$\forall i\in \lbrack 0,M)$

\qquad $\forall j\in \lbrack 0,N)$

\qquad \qquad n=i-j

\qquad \qquad if ($n\in \lbrack 0,M)$)

\qquad \qquad \qquad $y_{i\,}+=x_{n}\cdot h_{j\,}$

This filter simply equates to the mathematical function: $x\ast
h=\sum_{l}x_{k-l}h_{k}$, which is the convolution operation. As we can see
the operation is slightly less than $O(n^{2})$ . For practical use, the
filter is made smaller than the actual signal being analyized. In some
cases, the filter may be much smalled than the signal. Size of the filter
matters in the features that are being extracted from original signal.

In the case of this convolution operator, the limit is acutally M, not 8.
The value of M is the size of the original signal. Since the resulting
vector is the same as the original, the vector is said to be fully
qualified. Some forms are made half size discounting half of the information.

To perform a wavelet transform via convolution, each signal is convolved
twice. \ 

$A_{i}=A_{i-1}\ast V$

$D_{i\,}=A_{i-1}\ast W$

\qquad where

\qquad \qquad V is the scaling wavelet vector

\qquad \qquad W is the differencing wavelet vector

\qquad \qquad A is the average vector (scaled vector)

\qquad \qquad D is the difference component vector

\qquad \qquad $\forall i\in \lbrack 1,L)$ and $A_{0}=f$ which is the
original signal

\qquad \qquad L is the limit on the number resolutions that signal can have
based on the wavelet type.

\bigskip

\section{Walker Method and Convolution Method}

There is a comparison of this method to James S. Walker's method. This point
is to be illustrated by the Haar wavelet. \ Note that other wavelet basis
functions may be different. \ By definition, the difference vector elements
are defined:

$d_{k}=f\cdot W_{k}$

\qquad where

\qquad \qquad $d_{k}$ is the difference value

\qquad \qquad $W_{k}$ is the wavelet vector shifted by (k-1)*2

\qquad \qquad $k\in \lbrack 1,\frac{m}{2}]$ such that m is the length of f

\qquad \qquad f is the original signal

The average vector elements are defined by $a_{k}=f\cdot V_{k}$

\qquad $a_{k}$ is the average value

\qquad $V_{k}$ is the scaling vector shifted by (k-1)*2

\qquad $k\in \lbrack 1,\frac{m}{2}]$ s.t. m is the length of f

\qquad f is the original signal

\bigskip 

Furthermore, the inverse of the wavelet transform defines the original
vector as 

$f_{2k}=a_{k}-d_{k}$

$f_{2k-1}=a_{k}+d_{k}$

\bigskip 

This example implies that each $A^{i}$ and $D^{1}$ are both half the size of
f in the James S.\ Walker method. \ Likewise, $A^{2}$ and $\ D^{2}$ are half
the size of $A^{1}$.

\bigskip 

For the convolution method for zero indexed vectors, the even indexed values
equated to the $a_{k}$ and $d_{k}$ values in Walker's method. \ Thus the
inverse via convolution is 

\qquad $f_{i}=a_{1}+d_{i}$ $\forall i\in \lbrack 0,m)$ that is even

\qquad $f_{i}=a_{i-1}-d_{i\,}$ $\forall i\in \lbrack 0,m)$ that is odd.

\bigskip 

Example:

To test these idea, a simple program was made to run this convolution
operator on simple functions. Haar matrices were generated by one class.
Convolution was placed in another class. Last, the representation in another
class. Special classes were provided for the test of the algorithm and
plotting of the vectors. 

The values chosen for the wavelet and scaling vectors were as followes:

\qquad $V=\sqrt{\frac{1}{2}}
\begin{array}{c}
1 \\ 
1
\end{array}
$

\qquad $W=\sqrt{\frac{1}{2}}
\begin{array}{c}
1 \\ 
-1
\end{array}
$

The start of the transform convolves the original signal into vector $R_{A}$
and $R_{D}$. \ From this a loop performs the operation of subsequent $R_{A}$
vector. \ In principle this should work. \ However, $R_{A}$ has a tendency
to grow.

The original signal for test was 

\qquad $f(t)=10\sin (t)-5\sin (\dot{2}\cdot t)+2\sin (3\cdot t)-\sin (4\cdot
t)$

The horizontal axis was represented on horizontal axis by $\forall t\in
\lbrack 0,128]$. \ The verticle access was represented on the vertical axis.

\bigskip 

The first average maintains the shape, but gains some strength: 

\bigskip 

Differences are notices mostly $\forall t\in (40,60)$. The rest are not so
remarkable

\bigskip 

One curious aspect of this example is that the average signal grows in each
iteration. In time is much larger in magnitude than the original. However,
the shape generally remains the same. Why is this? The first idea that comes
to mind is the scaling value on the scaling vector. Most filters are
supposed have loss of energy. However, there is an actual gain of energy
between the average versus the previous original. Why? 

Walker points out that a majority of the energy is contained with in the
average. Subsequent energy would be found in the average of the average of
the average ..... Unfortunately, these vectors are much the same size as the
original. 

Another reason could be the size of the convolution filter. This filter is
only 2 elements long. Where as the signal is 64 times its size. Could this
be contributing to this difference? 

\end{document}
