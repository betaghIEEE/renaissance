%\section {Wavelet Matrix Multiplication}
Wavelet based matrix multiply is sound for the Haar Wavelet Transform and the $\psi^n$ expansion.   In other words, All terms contributed by the wavelet transform on the operation of matrix multiplication are cancelled in the inverse.
%Wavelet based matrix multiplication is sound only as long as the wavelet operator is a linear operator.   Any optimizer applying the wavelet transform must ensure linearity is maintained.  
If the Haar Wavelet Transform produces a sparse and well conditioned matrix, then the Haar Wavelet Transform proves itself as a useful preconditioner.  %sparseness and a well conditioned matrix result in the process, then the resulting matrix should be simpler to multiply.    
In this chapter, the general concept of matrix multiplication via wavelets is introduced, and the linearity principle is shown.  
%One of t
The key point for wavelet matrix multiplication is the proof that $W(A) \times W(B) = W(A\times B)$  If this is the case, then it is obvious that $W(A) \times W(B) = W(C) = W(A\times B = C)$.  %So far the proof is still weak.  The reason is that an example proof is useful for proving something to not be the case, rather than being the case.   However, a simple example does show some intuitive steps that would be necessary for a proof.  

\section { Proof of Wavelet Matrix Multiplication}
The following is a proof that wavelet multiplication is theoretically sound. It is based upon the fact that both wavelets and matrix multiplication  are linear operators.
%A reminder from Dr. Sinzinger, wavelets and their inverses are linear operators.  As he correctly reminded me any linear operator has the following properties:

\begin{enumerate}\item $L(AB) = L(A) \dot L(B)$
\item $\psi ^{-1}$ is a linear operator
\item $\psi ^{-1} (\psi (A) \dot \psi (B)) = \psi ^ {-1} (\psi (A)) \dot \psi ^{-1} (\psi(B)) $
\item $\psi ^{-1}(\psi(A)) = A$
\item $\psi ^{-1}(\psi(B)) = B$
\item Therefore:  $AB = \psi^{-1} (\psi(A) \dot \psi(B))$  
\end{enumerate}

Thus this is sufficient proof that wavelet matrix multiplication is sound.  


\subsection{A $2\times 2$ example}

The feasibility of multiplication in the wavelet domain is demonstrated directly using a $2 \times 2$ matrix. The coefficients of the matrices are multiplied both according to normal matrix multiplication and the modified wavelet multiplication operator. In the end the resulting coefficients are seen to be the same.

\subsubsection{Conventional Multiplication}
Conventional multiplication is spelled out as
\[
c_{i,j} = \sum\limits_k a_{i,k} b_{k,j}.
\]

For a $2 \times 2$ matrix, this can be presented by
\[
\left(\begin{array}{cc}  a^1_1&  a^2_1 \\ a^1_2 &  a^2_2 \end{array}\right)
\left(\begin{array}{cc}  b^1_1&  b^2_1 \\ b^1_2 &  b^2_2 \end{array}\right) =
\left(\begin{array}{cc}  a^1_{1} b^1_{1} + a^2_{1} b^1_{2}&  a^1_{1}b^2_{1} + a^2_{1}  b^2_{2}    \\ a^1_{2} b^1_{1} + a^2_{2} b^1_{2} &  a^1_{2} b^2_{1} + a^2_{2} b^2_{2} 
\end{array}\right)
\]

\subsubsection{Wavelet Transforms of the Matrices}

For a wavelet transform, the result on matrix $A$ is
\begin{equation} \label{WA} \displaystyle
W(A) = 
{1 \over 2} 
\left(\begin{array}{cc}  
\left(a^1_1 + a^1_2 + a^2_1 + a^2_2\right) & 
\left(a^1_1 + a^1_2 - a^2_1 - a^2_2\right)  \\ 
\left(a^1_1 - a^1_2 + a^2_1 - a^2_2\right) & 
\left(a^1_1 - a^1_2 - a^2_1 + a^2_2\right)   
\end{array}\right), 
\end{equation}
and for matrix $B$ it is
\begin{equation} \label{WB} \displaystyle
W(B) = {1 \over 2} 
\left(\begin{array}{cc}
\left(b^1_1 + b^1_2 + b^2_1 + b^2_2\right) &
\left(b^1_1 + b^1_2 - b^2_1 - b^2_2\right) \\ 
\left(b^1_1 - b^1_2 + b^2_1 - b^2_2\right) &  
\left(b^1_1 - b^1_2 - b^2_1 + b^2_2\right)
\end{array}\right).
\end{equation}

\subsubsection{Product of $A$ and $B$ in wavelet space}

The conventional product of $A$ and $B$ can be transformed into wavelet space.  The product of the two matrices is
\[
A \cdot B = 
\left(\begin{array}{cc} 
\left(a^1_1b^1_1 + a^2_1 b^1_2 \right) &  
\left(a^1_1b^2_1 + a^2_1 b^2_2 \right) \\ 
\left(a^1_2 b^1_1 + a^2_2 b^1_2\right) &  
\left(a^1_2 b^2_1 + a^2_2 b^2_2\right) 
\end{array}\right)   .
\]
The wavelet transform of this matrix is represented by 
\[
W(A \cdot B) =
{1 \over 2}
\left(
\begin{array}{cc}
\psi(A) & \psi(V) \\
\psi(H) & \psi(D)
\end{array}
\right)
\]
where 
\begin{eqnarray*}
\psi(A) &=& (a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2) + (a^1_2 b^1_1 + a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2)\\
\psi(V) &=&(a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2) +  (a^1_2 b^1_1 + a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 ) \\
\psi(H) &=& (a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2) - (a^1_2 b^1_1 + a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2)\\
\psi(D) &=& (a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2) - (a^1_2 b^1_1 + a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 )
\end{eqnarray*}
which simplifies to 
\begin{eqnarray*}
\psi(A) &=& a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2 + a^1_2 b^1_1 + a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2 \\
\psi(V) &=& a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2 +  a^1_2 b^1_1 + a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 \\
\psi(H) &=& a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2 - a^1_2 b^1_1 - a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 \\
\psi(D) &=& a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2 -a^1_2 b^1_1 - a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2  
\end{eqnarray*}

\subsubsection{The product of the waveletized matrices}

Straight forward multiplication of $W(A) \cdot W(B)$ represented by equations \ref{WA} and \ref{WB} works out as follows:
\[
W(A) \cdot W(B) = 
{1 \over 4} 
\left(
\begin{array}{cc}
W_A & W_V \\
W_H & W_D
\end{array}
\right)
\]
where
\begin{eqnarray*}
W_A &=& (a^1_1 + a^1_2 + a^2_1 + a^2_2)( b^1_1 + b^1_2 + b^2_1 + b^2_2) + ( a^1_1 + a^1_2 - a^2_1 - a^2_2)(b^1_1 - b^1_2 + b^2_1 - b^2_2) \\
W_V &=& ( a^1_1 + a^1_2 + a^2_1 + a^2_2) ( b^1_1 + b^1_2 - b^2_1 - b^2_2) + (a^1_1 + a^1_2 - a^2_1 - a^2_2) ( b^1_1 - b^1_2 - b^2_1 + b^2_2)  \\
W_H &=&  ( a^1_1 - a^1_2 + a^2_1 - a^2_2)(b^1_1 + b^1_2 + b^2_1 + b^2_2) +  ( a^1_1 - a^1_2 - a^2_1 + a^2_2 ) (b^1_1 - b^1_2 + b^2_1 - b^2_2) \\
W_D &=& ( a^1_1 - a^1_2 + a^2_1 - a^2_2) (b^1_1 + b^1_2 - b^2_1 - b^2_2)+( a^1_1 - a^1_2 - a^2_1 + a^2_2 )(b^1_1 - b^1_2 - b^2_1 + b^2_2)
\end{eqnarray*}
which simplifies to
\begin{eqnarray*}
W_A &=& a^1_1 b^1_1 + a^1_2 b^1_1 + a^2_1 b^1_2 + a^2_2 b^1_2 + a^1_1 b^2_1 + a^1_2 b^2_1 + a^2_1 b^2_2 + a^2_2 b^2_2 \\
W_V &=& a^1_1 b^1_1 + a^1_2 b^1_1 + a^2_1 b^1_2 + a^2_2 b^1_2 - a^1_1 b^2_1 - a^1_2 b^2_1 - a^2_1 b^2_2 - a^2_2 b^2_2 \\
W_H &=& a^1_1 b^1_1 - a^1_2 b^1_1 + a^2_1 b^1_2 - a^2_2 b^1_2 + a^1_1 b^2_1 - a^1_2 b^2_1 + a^2_1 b^2_2 - a^2_2 b^2_2 \\
W_D &=& a^1_1 b^1_1 - a^1_2 b^1_1 + a^2_1 b^1_2 - a^2_2 b^1_2 - a^1_1 b^2_1 + a^1_2 b^2_1 - a^2_1 b^2_2 + a^2_2 b^2_2 
\end{eqnarray*}
This can then be compared to the coefficients of $W(A \cdot B)$ which were
\begin{eqnarray*}
\psi(A) &=& a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2 + a^1_2 b^1_1 + a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2 \\
\psi(V) &=& a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2 +  a^1_2 b^1_1 + a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 \\
\psi(H) &=& a^1_1 b^1_1 + a^2_1 b^1_2 + a^1_1b^2_1 + a^2_1  b^2_2 - a^1_2 b^1_1 - a^2_2 b^1_2 - a^1_2 b^2_1 - a^2_2 b^2_2 \\
\psi(D) &=& a^1_1 b^1_1 + a^2_1 b^1_2  - a^1_1b^2_1 - a^2_1  b^2_2 -a^1_2 b^1_1 - a^2_2 b^1_2 + a^1_2 b^2_1 + a^2_2 b^2_2  
\end{eqnarray*}
Notice that $W(A) \cdot W(B) = W(A \cdot B) $,  in the case of $2 \times 2$ matrices.

\section{Chain Multiplication Structure}
%Recall the chain structure.
Most introductory texts of computer algorithms contain a section on the chain structure under topic of searching.  %This structure can be referenced from most introductory algorithm books.  
The chain structure  is used to %be able to
locate specific items quickly by some key which reduces the search to a family of keys that close together by a hashing scheme.  In the case of matrix multiplication, no hash key is needed.  The row or column identifier is sufficient for this.  Of course a description of this scheme is in order.  

Each chain is organized as a one-dimensional array.  Each array has an array list, called a link, which contains the actual data, and array list count showing how many items are in each array.  A next and previous value in this case would be superfluous since columns and rows are arranges in lexicographical order.  The array list contains the following items:
\begin{itemize}\item key
\item item value (row value/ column value)
\item previous key
\item next key \end{itemize}

In the case of a column chain, the keys are row identifiers.   In the case of a row chain, the keys are column identifiers.  This structure is similar to a matrix and can be represented by a matrix.  The essential thing for the chain is the arrangement of the array lists.  Zero values are not allowed, and thus order and index keys must be maintained.  

%What has this to do with sparse matrix multiplication?
The Chain Structure and sparse matrix multiplication have a common theme.  The left matrix is transformed into a row chain and the right matrix is transformed into a column chain.  Each value of the result matrix is simply a multiplication of the row vectors in the left matrix by the column vectors in the right matrix.  With the chain structure, %it simply
the operation is simply multiplying the row links times the column links.  Only elements with matching keys are allowed to be multiplied together.  The rest are assumed to be zero, and thus no action is taken.  The sum of the multiplied elements is the result.  The multiplication procedure is as follows:

\begin{enumerate}\item Chain Multiply
\begin{enumerate}\item Arguments: left matrix chain (A) and right matrix chain (B)
\item Results: Result matrix\end{enumerate}
\begin{itemize}\item $\forall i \in R.row$
\begin{itemize}\item $\forall j \in R.col$
\begin{itemize}\item $R_{i,j} = CM( A[i], B[j]) $  
\item ---- Note that CM is the chain multiply procedure.  \end{itemize}\end{itemize}\end{itemize}

\item Chain Multiply Element
\begin{enumerate}	\item Arguments:  row link (r) and column link (c)
	\item Output:  Double result: total	\end{enumerate}
	\begin{itemize}		\item rlimit = r.size;
		\item climit = c.size;
		\item k = l = 0;
		\item jlow = 0;
		\item total = 0.0
		\item BnotExhausted = true
		\item while both ( k < rlimit) and (BnotExhausted)
		\begin{itemize}
			\item $\forall l \in [jlow, climit)$ if ( $A^c_i . getkey(k) \equiv B^c_j getkey(l)$) then lmatch = l
			\item if ( $l \equiv climit$) BnotExhausted = false
			\item otherwise
				\begin{itemize}					\item temp += $A^c_i[k] * B^c_i[lmatch]$
					\item jlow = lmatch				\end{itemize}		\end{itemize}	\end{itemize}\end{enumerate}
					
For the Matrix Chain Multiply the complexity is $O(N^2)$.  For the chain multiply, the complexity is $O(M)$ where M is the larger length of the two links.  Thus total complexity is $O(N^2 M)$ since M's largest size is N.  This a general and simple algorithm for sparse matrix multiplication of matrix chains.  Of course the chore of loading the matrix chains is $O(N^2)$ per matrix.  Thus total cost is on the order of $O(3N^2 M)$.  So long as M is significantly less than N ( on the order of a $1 / 3$), then the wavelet matrix multiplication has a reasonable advantage.  

\section{Practical Implementation}

There are a few questions about the practical implementation of the wavelet based matrix multiply operation that must be answered:
\begin{enumerate}\item Are the matrices the same size and are they each a square matrix?
\item If not, are the dimensions suitable for multiplication?
\item If so, what is the maximum resolution for each matrix?  The lesser maximum is the limit for both.
\item Is the wavelet transform performed on each matrix the same?\end{enumerate}

% Proofed up to this point as of November 10, 2003

In case of the matrices both being of the same size and both being square, %A yes answer to the first matrix, allows for 
it is relatively easy to transform and multiply the matrices.% multiply.
  If the matrices are not square, then practical problems exist.  If the wavelet transform is the same on each matrix, then the linear proof is sound.  However, if they are not, then the proof is bogus and the multiplication is too.  

The general wavelet based matrix multiply is as follows:
\begin{enumerate}\item Arguments:  
\begin{itemize}\item A :  a $m \times p$ matrix
\item B: a $p \times n$ matrix\end{itemize}
\item Results:  C : a $m \times n$ matrix
\item Procedure:
\begin{itemize}\item $ A \stackrel{\psi}{\to} \alpha$
\item $ B \stackrel{\psi}{\to} \beta$
\item $ \alpha \stackrel {Chain Row}{\to} \alpha ^c$
\item $ \beta \stackrel {Chain Column}{\to} \beta ^c$
\item Chain Multiply ($\alpha ^c , \beta ^c$) $\to C$ \end{itemize}\end{enumerate}\subsection{Chain Row and Chain Column Setup}Setting up the the chain-link structure for row or column orientation is relatively simple.  Each structure requires proper addressing of the hooks.  The procedures are as follows:

\subsubsection{Row Chain-Link Setup}
\begin{itemize}\item $\forall i  \in \alpha .rows$
\begin{itemize}\item $\forall j \in \alpha . columns$
\begin{itemize}\item if ( $\alpha [i][j] \approx 0 $)
$a^c.hook_i . addlink(j,\alpha_{i,j} ) $\end{itemize}\end{itemize}\end{itemize}
\subsubsection{Column Chain Link Setup}
\begin{itemize}\item $\forall i  \in \alpha .rows$
\begin{itemize}\item $\forall j \in \alpha . columns$
\begin{itemize}\item if ( $\alpha [i][j] \approx 0 $)
$a^c.hook_i . addlink(j,\alpha_{i,j} ) $\end{itemize}\end{itemize}\end{itemize}

One of the big questions is how to optimize wavelet packets for multiplication applications.  A straight multi-resolution wavelet on a square matrix is seemingly trivial.  In the square matrix, straight multi-resolution case the number of resolutions is dictated by the size of the matrices being multiplied.  For wavelet packets on square matrices, it is also a size issue.  One can apply wavelet packets to the maximum extent that the matrix size allows.  However, some of the packet levels may be unnecessary since the amount of energy shuffling is small in comparison to the number of operations.  In the case of non-square matrices, best fits must be applied to ensure that the same wavelet transforms are applied to both matrices. 

\subsubsection{The Chain-Link Structure}

The chain-link structure is derived from two other classes (hooks and links).   Each one of these classes are relatively simple and can be implemented with either arrays or pointers.  Due to the anticipated size, the array mechanism is chosen for speed and efficiency.  

Class Chain-Link members
\begin{itemize}
\item length 
\item hooks\end{itemize}

Class Hook 
members:
\begin{itemize}\item length
\item l2norm
\item links \end{itemize}


Class Link
members: 
\begin{itemize}\item id
\item value\end{itemize}


\subsubsection{Recursive Wavelet Packets}
The algorithm for optimal wavelet packets is easiest to describe by recursive means.  The stop condition is either when the maximum resolution has been reached or when the energy shuffle metric is satisfied.  Determining the optimal condition is a matter that is not well-defined.  The wavelet packet procedure is as follows: \\
%\begin{itemize}Procedure Wavepack (matrix A, integer res) return $\mu$
\begin{itemize}\item if ($res \equiv 0$  ) stop
\item if ( $A.row {modulo} 2 \equiv  0$ and $A.col {modulo} 2 \equiv 0$) stop
\item if ( optimum) stop
\item otherwise 
\begin{itemize}\item $A \stackrel{\psi}{\to} \mu $
\item $\mu \stackrel{topleft}{\to} \alpha$
\item $\mu \stackrel{lowleft}{\to} \beta$
\item $\mu \stackrel{topright}{\to} \gamma$
\item $\mu \stackrel{lowright}{\to} \delta$
\item wavepack ( $\alpha$ , res -1)
\item wavepack ( $\beta$ , res -1)
\item wavepack ( $\gamma$ , res -1)
\item wavepack ( $\delta$ , res -1)
\item insert ($\alpha  \stackrel{topleft}{\to} \mu$)
\item insert ($\gamma  \stackrel{topright}{\to} \mu$)
\item insert ($\beta  \stackrel{lowleft}{\to} \mu$)
\item insert ($\delta  \stackrel{lowright}{\to} \mu$)
\item stop\end{itemize}\end{itemize}%\end{itemize}


%The determining of the optimum condition is matter that is not well defined. 

% The big question is how does $||\psi ^{-1} (\psi (A) \psi (B) ), AB ||$

