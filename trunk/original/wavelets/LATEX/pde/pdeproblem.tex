
%In most of the literature, 
Wavelets are used to condition the difference equation matrix and yield a %better conditioning of the 
sparser matrix.  What this conditioning means is that a wavelet enhanced matrix is easier to solve.  How is this enhancement accomplished?  The condition number is an indication of how quickly a solution will converge.    Sparseness refers to the large concentration of zeroes in the matrix.  Consequently, a sparse matrix has fewer elements to be used in a computation.  The point of this chapter is show how wavelets can be applied to yield a well-conditioned and sparse matrix, how conventional methods of solving PDEs work,  and why a wavelet transformed matrix works for the solution.  Also included is a comparison between conventional and wavelet based methods.  

%This is useful %in
%for increasing the likely hood that the matrix can solved.  %Also, for some implicit forms, 
%Sparse matrices, like those generated by the wavelet transform, are less complex and easier to solve in the case of implicit forms.  %Wavelets can make the matrix even more sparse.  
%When this is the case, 
%In these cases, fast algorithms %can be 
%The reason is that sparse and well conditioned matrices allow fast algorithms to be applied.
%are applied.  
%The goal of this chapter of this thesis is to explore these %softening
%sparse matrix producing  and conditioning techniques that wavelets offer in comparison to traditional methods.  

%\section {PDE in General}
\section {Overview of PDE}
What are partial differential equations (PDE) good for?  
Partial Differential Equations are used to describe several types of systems.  
%Computational Scientists consider partial differential equations (pde) important their ability to 
% PDE could use a s to enforce plurality 
PDE are used to solve %problems as they relate 
equilibrium, diffusion states, and oscillitory systems.  Most %of the
conventional algorithms are computationally expensive %costly 
for performing these tasks.  Therefore the challenge is computing %how to compute 
the solution of PDE with greater speed and accuracy.
%faster.
%these faster has computational scientist concerned.  This why the use of wavelets are considered.  


\subsection {Classic Methods}
%In the classic sense computational scientists agree that 
There are three basic %classification
categories of partial differential equations: elliptical, parabolic, and hyperbolic.  Some scientists consider the hyperbolic to %be made of 
contain a fourth category, ultra-hyerpbolic.  %Regardless, 
Similar properties exist for these groups of problems.  Properties include their boundary conditions and the physical phenomena the PDE models.  %These kind of problems are grouped together for the similarities in properties and boundary conditions. % Also, these classes of PDEs have their own methods for solving them, which are subtly different from each other.  
%Also, three of these types have their own usefulness as they model physical phenomena.  

%For example, 
Elliptical PDE represent %have examples in 
heat expansion, electrostatic charge, and other source expansion problems.  Parabolic PDE model diffusion of gases or fluids.  Oscillating states such as electromagnetic fields, some control theory, vibrating strings, and electronic communication systems all map to the model of hyperbolic PDE.%involve hyperbolic PDEs to model their behavior.  

The general form of second order partial differential equations is %difference for second order partial differential equations is defined % from their general form:

\begin {equation}
Au_{xx} + Bu_{xy} +Cu_{yy} +Du_{x} + Eu_{y} + Fu = G
.
\end {equation}


For the case where $B^2 - 4AC > 0 $, %then 
the PDE is hyperbolic, and the solution has an oscillatory nature to it.  In cases where  $B^2 - 4AC < 0 $,  %then 
the PDE is elliptical.  If $B^2-4AC =  0$ %there is an equivalence to zero, then
the PDE is parabolic.  These classifications determine how the PDE is represented and solved in both the analytical and numerical cases.  %The effect this has on classic methods (analytical and numerical) is how the PDE is represented and eventually solved.  


In addition to the above classifications, %the means of 
their limits or boundaries also have a classification.   Boundary conditions are defined either on the dependent variable or the gradient of the dependent variable.  If %it is the case that 
the specified boundary values produce an open region,  %If the boundary conditions leaves the region open, 
then the problem is an initial value problem.   %These classifications are defined on the method of dependent variable definition.  
%The following are those categories:
%\begin{itemize}%\item Elliptical (Equilibrium States)
%\item Parabolic (Diffusion states) 
%\item Hyperbolic (oscillating or vibrating states)
%\item Ultra-Hyperbolic %\end{itemize}


Boundary conditions for %elliptical
PDE%s
 consist of three categories %check spelling: 
 Dirichlet, Neumann, and Robbin's.  Dirichlet Boundary conditions are specified by either functions or constants on the solution function itself.  Neumann Boundary conditions specify against the derivative of the function based on some either independent or dependent variable.  The combination of the two is called Robbin's Boundary Conditions.  %What does mean?

The following are examples of Dirichlet, Neumann and Robbin's method, respectively% order:
\begin{itemize}\item $u_n =g(t) $
\item $u = g(t) $
\item $u_n + \lambda u = g(t)$
\end{itemize}



%Subtle differences exist in the solutions to these problems and due to nature of  the problems.  
Each solution (analytic or numerical) is tailored to the equation; however, the three main categories will have similar solutions due to similarities in the problem. 
%can be tailored to fit the specific equation.  
In numerical terms there are two general mechanisms for solving these equations: explicit and implicit.  Explicit tends favor initial value problems and has difficulties with round off errors.  Implicit handles error quite well; however, complexity %exists
 increases depending on the number points to be solved for.  A third method also exists, Monte Carlo.  However, the Monte Carlo method is more for simulating the results of differential or integral equations.  




\subsubsection {Elliptical Differential Equations} 
%One example type that shows up for Elliptical PDE problems is heat conduct, gravitational, electric field, and other static problems.  
Elliptical PDEs can be represented with Poisson's formula.  In the homogenous case, Laplace's equation is the representing equation:
%in the homogeneous case.

\begin{equation}\frac{\partial ^2 T} {\partial x^2} + \frac{\partial ^2 T} {\partial y^2} = 0\end{equation}


Also, it is typical for a polygon to be chosen to define the boundary conditions.   Typically, the polygon is defined by constants in either the first derivative or normal space.  %An example would be
The simplest case is a rectangle specified in the first derivative for the right and top edges and constants for the lower and left edges.  Also, the central difference formula is used to setup a linear set of equations to be solved.  \cite{appliedmethods}.  The trick is to set up the matrix and then solve it. % by any means (classic or wavelet based).  

%Boundary conditions for %elliptical
%PDEs consist of three catagories: Dirichlet, Neumann, and Robbin's.  Dirichlet Boundary conditions are specified by either functions or constants on the solution function itself.  Neumann Boundary conditions specify against the derivative of the function based on some either independent or dependent variable.  The combination of the two is called Robbin's Boundary Conditions.  What does mean?

Boundary conditions define the starting point and limits for solving PDEs.  This condition is necessary to have enough constants in the solution for a solution to be calculated.  Different boundary condition types have different methods of solution to account for the boundary conditions.  

\subsubsection {Parabolic Differential Equations}
The parabolic type of differential equations usefully describe diffusion and fluid mechanics.  Some analytical methods useful for solving parabolic equations include substitution of variables as well as Laplace and Fourier Transforms.  Examples of parabolic equations include heat diffusion, diffusion - convection, and Navier-Stokes.  

%Boundary conditions for parabolic PDEs are less formal than elliptical PDEs.  Boundary conditions are specified either across the boundary, on the boundary, or a hybrid of the two.    The boundary conditions are typically spelled by example in temperature equations.  However; it is conceivable that other diffusion problems have similar issues.  

\subsubsection {Hyperbolic Differential Equations}
The hyperbolic type of differential equations are prevalent in equations for oscillating phenomena.  This includes electromagnetic fields and vibrating strings.   In its analytic %al 
form, the D'Alembert Solution is an textbook example method.  The D'Alembert Solution is an example of the canonical form and is used in solving PDE problems.  

% Proofed up to this point 16:07 Friday 

In some literature the canonical form is used to generate a sparse domain.  The general rule for using the canonical form comes from the D'Alembert Solution which is as follows:
\begin{enumerate}\item Replace (x,t) by new canonical coordinates.
\item Solve the transformed equations.
\item Transform the solution into the original coordinates
\item Substitute the general solution into the initial conditions (IC) and %the
 acquire the constants. \end{enumerate}

%\subsubsection {PDE Boundary Value Problems}



 

Some of the boundary conditions that Farlow \cite{PDEfSE} refers to are controlled end points, force specified boundaries, and elastic attachments.  For these force specified boundary conditions, %in the case of force specified points,
 the boundary point itself can move in position.  In the elastic attachment problem, the force can change on each of the boundary points.  %While, 
 %Controlled end points are straight forward functions or constants.  % page 147
Controlled end points are constants or functions on the solution.   
In each case, the force, position, and function of the boundary conditions must be accounted for through out the solution of the PDE.   It is unknown yet as to whether this information will be used in solving any problems in the thesis itself.  

\subsection {Solutions to PDE}

\subsubsection {Difference Equation Formulae}
There are a few formulae used to convert PDE in to their difference equation form.    These formulae are a necessary first step to make a differential equation computable.  The type of of difference formulae is determined by the type of problem.   Each of these come from limits taken to include an $h$ value that moves the index.  If the formula is based on an $+h$ then the forward difference formula is the natural result.  Likewise, $-h$ is the natural result for the backward difference formula.  Central difference is a hybrid of both.  For Central Difference there is a formula for the second derivative as well.  

Central Difference Formula:
\begin{enumerate}\item \[f'(x) \approx \frac{f(x+h) - f(x-h)}{2h} \]
\item \[f''(x) \approx \frac {f(x+h) -2f(x)+ f(x-h)}{h^2}\]\end{enumerate}

Backward Difference Formula: 
\[f'(x) \approx \frac{f(x) - f(x-h)}{h} \]

Forward Difference Formula
%\begin{itemize}%\item 
\[f'(x) \approx \frac{f(x+h) - f(x)}{h} \]%\end{itemize}


\subsubsection {Explicit Methods/ Iterative Methods}
Explicit methods compute the solution of PDE by using the previous result to compute the current result.  The first step is to  %is 
%to 
acquire %the solution %is%
the transform of the PDE %to
in  its difference equation equivalent.   Any of the difference equations can be used to acquire this form.  %What is important?  
The one line system of eqaution and initial value formula are critical for setting up the remaining solutions. %are used to calculate one result after the next of partial differential equations.  One thing that both implicit and explicit methods have in common is to transform the PDE in difference equations.  At the heart of both methods is the central, backward, and forward difference approximation formulae.  The central difference method is applied to part of a grid with accuracy of $O(x^2)$ and $O(t^2)$.  To illustrate the explicit method, a parabolic PDE used for reference.  That PDE is as follows: %An example of a parabolic equation is used to illustrate this method.  When substituted into the simplest parabolic form;  the new form is 

%Example Method:
%\begin{itemize}%\item $\phi _{i,j+1} = \phi _{i,j+1} + \frac{2\alpha^2 \Delta t} {\Delta x^2}  (\phi _{i+1,j} -2 \phi _{i,j} + \phi _{i-1,j})   $
%\item $\phi _{i,j+1} = p \phi _{i+1,j} + q \phi _{i,j} + r \phi {i-1,j} $ such that $p,q,r > 0$ and $ p + q + r \le 1$%\end{itemize}
%$\phi _{i,j+1} = \phi _{i,j+1} + \frac{2\alpha^2 \Delta t} {\Delta x^2}  (\phi _{i+1,j} -2 \phi _{i,j} + \phi _{i-1,j})   $

%$\phi _{i,j+1} = p \phi _{i+1,j} + q \phi _{i,j} + r \phi {i-1,j} $ such that $p,q,r > 0$ and $ p + q + r \le 1$.  

One method is defined for two independent variables.  The solution is started from the set initialized points.  These points already have values as specified by the problem.  The solution is computed from these values.  Then next set of boundary conditions are calculated relative to the next solution.  This procedure is followed until the field is exhausted.   

If the x and t are independent variables, then the following outline is a more specific example of the explicit method.  
\begin{enumerate}\item Start with the index value of 0 (i=0 at the initial value)
\item Find the solution for all of x for $t=t_{i+1} $ by explicit formula.
\item Establish the boundary conditions with respect for $u_{t_{i+1} ,x}$ by boundary condition approximation formula.
\item Repeat steps 2 through 4 until i = n (where n is the maximum index).\end{enumerate}

%Typically the explicit formula is composed of the central difference formula for the PDE being solved.  The boundary condition formula is established as the difference formula of all boundary conditions which are not constants.  

%In this case the 
The explicit method is used to solve a PDE where parts of it have been transformed into its central difference equivalent.  The boundary conditions must also be computed in cases where they are not constants.  In cases where the boundary conditions are specified on a derivative, those conditions must be placed in their difference equation form.  %Note that for each iteration, the boundary condition must be computed.

The strength of the explicit method is its speed.  It is a top-down dynamic algorithm which uses only a few previous results to compute the next result.  Thus each result is computed quickly.  However, the
%This computation is useful for cases where speed is necessary.  This method's speed comes from the next step being computed from the current and previous steps.   However, the %is 
explicit form is unstable, and tends to yield results inconsistent with the boundaries.   In order for this form to be stable a reasonable $\Delta x$ and $\Delta t$ must be chosen.  


%Limits on the explicit Method and alternatives by implicit methods are as follows:  
%\begin{enumerate}%\item Imposed limits on $\Delta x$ and $\Delta t$.  Dependences in explicit methods are directly limited to 3 values of the many values which it theoretically should.
%\item The implicit method is approximately the second derivative $\frac{ \partial ^2 \theta }{\partial x^2} |_{i,j} $ ``by the finite difference formula involving $\theta$ at an advanced time ($t_{j+1}$)''  \cite {appliedmethods}.  A mid-point is computed using a central - difference formula 

%	$\frac{\partial \theta}{\partial t} | _{i,j+1/2} = \frac {\theta_{i,j+1}  \theta_{i,j} }{\Delta t}$
	
%\item The 2nd p.d. applied with central-difference formula.  There is catch with a weighting parameters.  
%\item Variable Weighted Implicit Formula can be used with the following conditions:
%\begin{itemize}%\item weighting factor $\theta$
%\item more than one unknown variable at the time step $ j+1$%\end{itemize}%\end{enumerate}

% Proofed up to this point Nov 3 10:05

\subsubsection{Implicit Methods}

One area where explicit and implicit methods differ is the arrangement of the linear equations used to solve the system.  %Typically, explicit methods utilize the previous solution to determine the boundary conditions of the current solution.   
While in a sense both use linear equations, implicit methods are computed as a system of equations. %lend themselves to simultaneous solution more than explicit ones.    
The implicit method can be computed for a grid of any size.  This is different from the explicit method where the change in value from one index to the next has to be within a narrow range.  

An example of an implicit method is the Crank-Nicolson Method,  a %The idea is to solve by use of a 
system of equations arrived at by converting the PDE into a system of difference equations.   

The Crank-Nicolson Method is performed by the following steps:
\begin{enumerate}\item Pick some value for $\lambda$ such that $\lambda \in [0,1] $
\item Pick $\Delta x$ and $\Delta t$ and assign grid points
\item Use computational molecule to generate equation.
\item Solve the matrix\end{enumerate}



\subsubsection {Galerkin}
There is a special case of the implicit method which attempts to optimize the matrix before solving it.  The Galerkin method supposes that a complete orthonormal system $\{v_j\}_j$ is defined on $L^2([0,1])$ and every $v_j$ is $C^2$ on [0,1].  The boundary conditions of the $v_j$ are defined as well.  %is typically defined as well.  
The solution approximation is then defined on the span of this orthonormal system.  For example,
$u_s = \sum_{k\in \Lambda} (x_k v_k )\in S$ such that S is a span of $v_j : j\in \Lambda$, $\Lambda$ is a subset of the natural numbers and $x_j$ is a scalar.  %The catch
However, the necessary condition is that $u_s$ should behave as true solution a system of linear equations, i.e. a vector itself.   The linear equations are %is 
the implicit set of equations for solving a PDE or ODE.  

Frazier takes this one step further to show a parallel from Galerkin to a conventional implicit form.  If L is a linear operation then %First he shows:
%\begin{itemize}%\item $\langle L u_s, v_j \rangle = \langle f, v_j \rangle$ $\forall j\in \Lambda$ such that $\langle f, g \rangle = \int ^1 _0 f(t) \bar{g(t)} dt $ 
%\item Furthermore: $\langle L (\sum_{k\in \Lambda} x_k v_k), v_j \rangle = \langle f, v_j \rangle$ $\forall j\in \Lambda$ leading to
%$\sum_{k\in \Lambda} \langle L v_k , v_j \rangle x_k = \langle f, v_j \rangle$ $\forall j\in \Lambda$ %\end{itemize}
$\langle L u_s, v_j \rangle = \langle f, v_j \rangle$ $\forall j\in \Lambda$ such that $\langle f, g \rangle = \int ^1 _0 f(t) \bar{g(t)} dt $ .
Furthermore: $\langle L (\sum_{k\in \Lambda} x_k v_k), v_j \rangle = \langle f, v_j \rangle$ $\forall j\in \Lambda$ leading to
$\sum_{k\in \Lambda} \langle L v_k , v_j \rangle x_k = \langle f, v_j \rangle$ $\forall j\in \Lambda$ .

%Consistent method of introducing equations Proofed upto this point November 4, 2003

The final connection is that each element of a matrix A defined as $A=(a_{j,k} )_{j,k \in \Lambda}$ is a scalar defined by  $\langle Lv_k , v_j \rangle$.  This connection yields the following equality:

\[(a_{j,k} )_{j,k \in \Lambda} = \langle Lv_k , v_j \rangle ,  \forall j\in \Lambda\] and \[\sum_{k\in \Lambda} a_{j,k} x_k = y_j,  \forall j\in \Lambda\]

The values of this equation are as follows:
\begin{itemize}\item x is a vector $(x_k)_{k\in \Lambda}$
\item y is a vector $(y_k)_{k\in \Lambda}$ 
\item $A$ is a matrix with rows and columns indexed by $\Lambda$
\item $a_{j,k}$ is an individual element of $A$
\end{itemize}

With Galerkin, for all subsets in $\Lambda$ we obtain an approximation $u_s \in S \to u$.  Such an approximation is obtained by %This is done 
 solving $Ax=y$ then using x to determine $u_s$.  The primary issue %One of the tricks
 is finding  the $v_j$'s and $x_j$'s such that the equations are satisfied.  

Frazier points %ed
 out that the Galerkin method produces a sparse and well conditioned %number
 matrix. %such that condition numbers range from one to infinity. 
   If a wavelet operator is applied, the idea is that the condition number and sparseness will be at a level such that convergence is assured for a given system.  %This is one of the hypothesis to be tested. 
   
  

\subsection {Application of PDE}
The previous subsection discussed PDE in general and how to solve them.  Listed in this subsection are some common PDE problems.  This section has at least one problem of each of the three PDE categories.  In this section, the classic method of solution is applied.  However, the wavelet methods are saved for the next section.  Implicit and Galerkin solutions are generally chosen where accuracy is required, and explicit methods are shown when speed and complexity are necessary.  

In order to make these solutions a few formulae need to be defined.  These formulae are the central difference, forward difference, and backward difference formulae.   Such definitions were provided in the section Difference Equation Formulae earlier in this section.  


\subsubsection {Semi-Infinite String Problem}
The semi-infinite string problem is a typical resonance problem.  Newton's physical laws derive the equation based on external forces, friction forces, restoration forces, and net forces.   In the simplest form, the problem is solved for net forces only.  %Even this problem requires a matrix to solve it.  
The problem is defined mathematically as:

\begin{itemize}\item PDE $u_{tt} = c^2 u_{xx}$  $\forall x \in (0, \infty)$ and $\forall t \in (0,\infty)$ 
\item BC $u(0,t) = 0$
\item IC $u(x,0)= f(x)$
\item $u_t (x,0) = g(x)$
\item general solution: $ u(x,t) = \frac{1}{2} [ f(x-ct) + f(x+ct)] + \frac{1}{2c} \int ^{x+ct}{x-ct} g(\zeta) d\zeta$
\item $c^2 u_{xx} $ is the net force due to the tension on the string
\item $u_{tt}$  represents the longitudinal or torsional vibrations on the string
\end{itemize}

There is a conventional solution that comes from the central difference, and forward difference formulae.  The rest is rather simple algebra.  One key issue is the boundary conditions.  Boundary values must %These must 
be solved to establish the constants in the matrix.  Once the boundary conditions %these 
are established, %the solution can be arrived at by conventional methods. 
conventional methods can produce the solution.   The conventional algebra is as follows:

\begin{itemize}\item $u_{xx} = u[i,j+1] - u[i,j] + u[i,j-1] $
\item $u_{tt} = u[i+1,j] - u[i,j] + u[i-1,j] $
\item $u_t = (u[i+1,j] - u[i,j]) $
\item $u[t,0] = 0 $
\item $u[0,x] = f(x) $
\item $u_t [0,x] = g(x) = u [1, x] - u[0,x] $
\item $u[1,x] = g(x) + f(x) $
\item $ u[i+1,j] + (c^2 - 1)u[i,j] + u[i-1,j] - c^2u[i,j+1]  - c^2 u[i,j-1] = 0 $\end{itemize}



\subsubsection {Heat Diffusion}
%Examples of two point diffusion problems are heat diffusion problems.   In this problem, time and location are the independent variables, and temperature is the dependent variable.  Such a model depends on two constants, the two diffusion points.  

The heat diffusion or heat conduction equation defines heat in a solid at any point and any time within the domain.  Diffusion comes from a heat source, and may come from an artifical constant.  In the case of heat diffusion, there is a constant $\alpha$ which is defined as the diffusitivity constant.  Diffusitivity is defined by the following:

\[\alpha = \frac{K}{\tau \sigma} \]

such that K is the thermal constant, $\tau$ is the density and $\sigma$ is the specific heat.  The heat diffusion problem is defined as follows:


\begin{itemize}\item $u_t = \alpha ^2 u_{xx} $ such that
\item $u_t$ is the change in temperature with respect to time
\item $u_{xx}$ is the concavity of the temperature
%\item Difference equations for this starts with $u_{xx} = \frac{1}{\Delta x} [u_{i,j+1} - 2u_{i,j} + u_{i,j-1}] $.  \end{itemize}

\subsubsection {Diffusion - Convection }
%The big idea for 
Diffusion - convection is a description of how %that %regards 
convection currents affect the diffusion process.  In particular, this concept addresses substances diffusing in areas where currents exist.  One case is a gaseous substance being released in an air conditioned room or wind area.  Another case is the release of a liquid substance into a river.  The general formula for diffusion convection is as follows:


\[ u_t = \alpha ^2 u_{xx} - vu_x \]  
where 
\begin{itemize}
\item %where 
$u_t $ is the change in time
\item $\alpha ^2 u_{xx} $ is the diffusion component
\item $vu_x$ is the convection component
\item v is the velocity of the convection current
\item $\alpha$ is the diffusitivity constant\end{itemize}

% Proofed November 5, 2003 .  Watch for this type intros

The solution to this problem is best done by considering the general solution, and then filling in the constants with the boundary conditions.  The general solution is as follows:

\begin{itemize}\item $u_t = \alpha ^2 u_{xx} - vu_x $
\item $u_t  =  u[i+1,j] - u [i,j]$
\item $u_x = u[i,j+1] - u[i,j] $
\item $u_{xx} = u[i,j+1] - 2u[i,j] + u[i,j-1] $
\item $u_t - \alpha ^2 u_{xx} - vu_x = 0$
\item $ u[i+1,j] - u [i,j] -  \alpha ^2 u[i,j+1] + \alpha ^2 2u[i,j] - \alpha ^2 u[i,j-1] - v u[i,j+1] + v u[i,j] =0$
\item $ u[i+1,j]   \alpha ^2 u[i,j+1]  - \alpha ^2 u[i,j-1] - v u[i,j+1] + (2 \alpha ^2 +v - 1)  u[i,j] =0$ \end{itemize}

%\subsubsection { Electromagnetic Fields Problems}

%\subsubsection {Electrostatic Expansion Problem}
\section {Related Work}

%\subsection {Gregory Beylkin}
\subsection {Sparsifying the Kernel with Wavelets}
One example of applied mathematician in the field of wavelets is Gregory Beylkin of the University of Colorado.  He has published several papers on the topic, and a few of his have similar work to this paper.  %Some of his quotes deserve some explaination, and other do not.  
This paper highlights a few of his quotes and explains their meaning.  The main point is to use the wavelet transform to diagonalize the matrix for fast sparse methods to work.  

One quote, ``Fast algorithms for applying these operators to functions, solving integral equations.  The operators which can be efficently treated using representations in the wavelet bases include Calderon-Zygmund and pseudo-differential operators.''\cite{bvpbeylkin}  This quote references two mathematical forms introduced in the early half of the 20th century.  Calderon-Zygmund was a difference operator that divided the vector in two each time it operated.  The more generic version, the psuedo-difference operator was introduced shortly thereafter.  Both introduced methods of numerically differentiating equations.  Beylkin used kernel K to illustrate the use of the wavelet transform: 

\[T(f)(x) = \int K(x,y) f(y)dy \]

This kernel allows for the mathematical explanation of how wavelets fit in scheme with %of 
solving PDE's.  The bottom line is best said in Beylkin's own words, ``If our starting point is a differential equation with boundary conditions then the wavelet system of coordinates there is a diagonal preconditioned which allows us to perform algebraic manipulations only with the sparse the sparse matrices. ... The wavelets play an auxiliary role in that they provide a system of coordinates where the condition numbers of the sparse matrices (involved in the computations) under control.  ''

Indeed, in Beylkin's examples the wavelets diagonalize the matrices for which it works on.  Beylkin's multi-resolution scheme is unique in the way it provides the diagonalization process.  %It 
Each resolution is performed successively on each average term.  However, the averaging and differencing is applied to portions in the vertical and horizontal components.  Thus, some of these parts are also forced to zero. % It is unknown as to why other multi-resolution techniques are not applied.  Also unknown is how determine an optimum diagonalization of the matrix by the wavelet transform.  

%Another paper of Beylkin had theme that for a number of operators, the non-standard form of wavelet basis could be used to reduce certain problems to a small system of linear equations.  It was highly mathematicall

Belykin claims in another paper that wavelets support rapid application of dense matrix.  In some cases, Beylkin claims to achieve results %operations
 in $O(N^2)$ operations.  In general, it is clear that wavelets offer the ability to reduce the overall size of the input, and therefore reduce the time necessary to compute the result.  It is important to note that the input is an equivalent form to the original, and the new form is simply more compact.  

On both integral applications, it is known that the algorithms have hidden recursive components that tend to make the algorithm grow exponentially.    In cases described by Farlow \cite{PDEfSE}, integral equations are solved in a similar manner to PDE's by using there numerical equivalence forms.  

Lastly, Beylkin points out that wavelets have several advantages over the Fast Fourier Transform (FFT).  Amongst them is the cost of the transform itself.  The wavelet transform in 1-D space has a cost of nearly $O (N)$. where the FFT is of order $O(N^2)$  Another is translation invariance.  The wavelet transform does not require translation invariance, where as the FFT does.  This gives the wavelet transform some adaptability to numerical applications.  




%\paragraph{Methods of the Paper}
%\subsubsubsection { }
%The paper used the Haar Wavelet Transform, and mechanisms first developed by Stromberg, Meyer, and Ingrid Daubechies.  Operators included in the study are integral, Dirichlet and Neumann boundary value problems for elliptical partial differential equations, and  Legendre series.  The idea like most wavelet schemes is to use the wavelet transform to make the matrix operator very sparse.  Once that is the case, the operation against a vector is $O(N)$ in complexity.      The construction is claimed to be $O(N^2)$ with the exception of structures whose singularities are known a priori.  In the case of the exception, the compression operator is an order O(N) procedure.  

%Mechanisms provided that the article claims to provide for evaluating integral operators.  A standard and non-standard form.  The non-standard scheme claims to extend the standard form leading to an $O(N)$ scheme.  

%The paper is organized with section concepts.  The first concept is the Haar Wavelet Basis.  Second are relevant facts regarding wavelets.  Third is a description of the integral operators for which we obtain an order N algorithm.  Included with the third is a billinear operator and its description.  Fourth is the complexity analysis.  Finally, the numerical applications of wavelets are presented.  

%\subsubsubsection
%\paragraph{Properties of Wavelets}
%One property that the article hits on is the fact the Haar Wavelet does not drop off very fast.  In the case of the paper, the Daubechies Wavelet Basis functions are used.  

%\section {Where do Wavelets Fit In}
%In most of the literature, wavelets are used to soften the difference equation matrix, and yield a better conditioning of the matrix.  This is useful in increasing the likely hood that the matrix can solved.  Also, for some implicit forms, Wavelets can make the matrix even more sparse.  When this is the case, fast algorithms can be applied.  The goal of this chapter of this thesis is to explore these softening  and conditioning techniques that wavelets offer in comparison to traditional methods.  

%\subsection {Softener }
%\subsection {Correction Factors}


\subsection {Turbulance and Navier Stokes}
Jaffard, Meyer, and Ryan showed exploration into the basis functions useful for solving PDE problems, such as Navier-Stokes and other turbulance problems.  Amongst the first attempts for solving Navier-Stokes used the Battle-Federbush and Shannon basis.  One of the keys for this problem was the absense of boundary conditions.  

The idea was to apply the Battle-Fererbush basis with the Gelerkin method to solve Navier Stokes.  This concept was first published by Zenin in 1981.  One of the difficulties is getting the matrix solution to diagonal form.  The Shannon basis does not cause the non-diagonal elements to decay rapidly.  In some cases, neither does Battle-Federbush.  

The main issues for wavelets and PDE solutions in some opinions is to address complicated geometries.  In some cases, %Others see that 
local refinements can enhance or be used in place of multi-grid algorithms% in some cases
.   Again, the key is to make the solution matrix converge to diagonal and sparse system.  

%\subsection {Wim Swelden}
%Wim Swelden has published papers of wavelets in the spherical coordinate system.  A few problems this addresses are PDE's on astronomical phenomina with consideration of the spherical system of coordinates.  One example of this was written by Gallegos, Martinez-Gonzolez, Argueso, Cayon, and Sanz.  The example was to solve non-Gaussian features in astronomical images and spectra.  In that case, the Spherical Mexican Hat basis was the best basis function for extracting the information.  

\subsection {KL Transform Approach}
%In the solution of the PDE in implicit form, the bottom line is to solve a linear equation.
All implicit methods place their information into a matrix and then solve that matrix as a system of linear equations.  One approach is to use the Karhunen-Loeve (KL) transform.   As stated by Wickerhauser \cite{victor} the overall goal of this solution method is to acquire the singular values, eigenvalues, and eigenvectors.  From these answers, the solution to the matrix is trivial.  Methods for accomplishing this extraction are factor analysis, principle component analysis, singular value decomposition, and the KL transform.  


%Proofed up to this point at November 6, 2003

The point of wavelets with the KL-Transform was well stated: ``The approximate factor analysis algorithm is the search through a library of orthonormal basis for the one whose H is closest to that of the Karhunen-Lo$\grave{e}$ve basis, and cases of fast search methods the result is a fast approximate Karhunen-Lo$\grave{e}$ve algorithm.  "\cite{victor}  

So what problem is solved by the KL transform?   ``Two problems solved by  principle orthogonal decomposition.  First, distinguishing elements from a collection by making d measurements.  Second, inverting a complicated map from a p-parameter configuration space to d-dimensional measurement space. '' \cite{victor}  What does this mean?  Usual cases for the extraction of singular values depend on the measurements taken.  The parameters necessary to get these values by certain formulae are extracted by their singular values that the KL transform can acquire.  In the case of PDE, these parameters represent a mapping to eigenvalues, eigenvectors, and singular values.  As stated, ``the Karhunen-Lo$\grave{e}$ve basis eigenvectors are also called principle orthogonal components or principal factors, and computing them for a given ensemble X is also called factor analysis.''\cite{victor}

Where do wavelets come in?   ``It is possible to build a library of more than $2^d$ fast transforms U of $R^d$ to use for ``x" points.''\cite{victor}%Question:  What does this mean?  
 This  built library means that wavelets can be used to construct the fast transforms family designated U which are specific for the point ``x.''  From these bases, a particular wavelet basis can be selected as the best choice. %,  or so this idea is implying.

%Look for plural of basis

The method of using wavelets to acquire the KL transform is stated as follows:  
 \begin{itemize}\item Expand N vectors $\{X_n \in R^d : n = 1,2, \ldots ,N\}$ into wavelet packets' coefficients: $O(Nd\log d)$
 \item Summing squares into the variance tree: $O(d \log d)$
 \item Searching the variance tree for the best basis: $O(d+d\log d)$, if it exists
 \item Sorting the best basis vectors into decreasing order $O(d \log d)$
 \item Diagonalizing the auto-covariance matrix of the top $d^\prime$ best basis vectors $O({d^\prime}^3)$\end{itemize}

The total complexity of the Approximate Karhunen-Lo$\grave{e}$ve bases: $O(Nd\log d + {d^\prime}^3)$.  Since $d^\prime \ll d$, it is safe to say the approximate solution is faster than the conventional one. 

The approximate Karhunen-Lo$\grave{e}$ve transform of one vector 
\begin{itemize}\item Computing the wavelet packet coefficients of one vector $O(d \log d)$
\item Applying the $d^\prime \times d^\prime$ matrix $K^{\prime \ast}$ : $O({d^\prime}^2)$\end{itemize}

Updating the approximate Karhunen-Lo$\grave{e}$ve basis 
\begin{itemize}\item Expanding one vector into wavelet packet coefficients 
\item Adding the coefficients into the means tree
\item Adding the squared coefficients into the squares tree
\item Forming the variance tree and computing the new information costs
\item Searching the variance tree for the joint best basis\end{itemize}
The classification in large data sets applies to both the rogues' gallery problem, fingerprint classification problems, and rank reduction for complex classifiers.  

%\subsection {Galerkin Approach}

%\subsection{The Recursive Multi-Resolution Approach}
\section {Proposed Work}
This chapter proposes to utilize the wavelet transform to pre-condition PDE.  Once a PDE matrix is preconditioned,  the similar matrix is to be solved.  A few wavelet basis functions and multi-resolution methods shall be studied for their properties in preconditioning a matrix.  

The PDE problems to be used are the semi-infinite string, heat-diffusion, and diffusion convection.    In all three cases, the implicit method enhanced with the wavelet transform shall be used.  A straight implicit method shall be used for comparison.  

\subsection {Basis Function Preconditioning Functions}
The primary basis functions to be used as test basis functions are the Haar Wavelet, Daub4, and the Mexican Hat.  The Haar basis function is orthonormal and preserves the data; however, Haar does not cause as rapid convergence as Daub4 or the Mexican Hat.  The Daub4 is bi-orthonormal and has a courser difference term.  %The Mexican Hat is under consideration due to its use in astrophysics on non-gaussian features. 

\subsection {Multi-Resolution Conditioning Methods}
There are two general forms that could be called the Recursive Multi-Resolution Wavelet Transform.  Both of these obey the rule from linear algebra that states that the solution to a matrix A is the same as that of matrix B if B is similar to A.   Where these two forms differ is in their coverage of the matrix.  It is presumed that these two forms can probably be applied to any orthonormal wavelet basis and probably to any bi-orthonormal wavelet basis as well.  

\subsubsection {Procession of Averages}
One form could be called the procession of averages.  It is a form that has been cited in this paper repeatedly.   Each resolution transforms the average term with another wavelet transform until the size of the final average term is too small on which to perform another transform.  %This works for 
The procession of averages works when most of the energy is contained in the average term.

\subsubsection {Recursive Multi-Resolution Quad-Tree Method}
An alternative form recursively works on all four terms.  A quad tree is used to model the recursive process, hence the name the Recursive Multi-Resolution Quad Tree.   In the case of the quad-tree, the stopping point is defined for either when a leaf's energy goes to zero or a leaf's matrix is too small to be transformed.  

An optimal condition is used as the stopping condition of the wavelet transform using the Recursive Multi-Resolution Quad-Tree Method.   %To achieve the primary objectives, an optimal condition has to be defined for the matrix and its quad-tree.  
This condition is used to zeroize the leaves of the quad tree as much as possible.  %The second is the concentrate the energy at the diagonal of the matrix.  %Last is just a hypothesis of using half-transforms.  
Another optimal condition that is desirable is for the energy of the matrix to be concentrated in the diagonal of the matrix.  However, this condition is not guaranteed by any multi-resolution method.   Of course, a matrix in any of the leaves which is not of size to be preconditioned is also a stopping condition.  

%The process of the wavelet transform should separate the high-energy components from the low/zero energy components.  Zero leaves are leaves where the energy of the leaf is below the zero energy threshold.  If this occurs the leaf can be labeled a zero leaf, and this is a terminating condition.  Obviously, non-zero leaves are significant and must be retained.  

%There is one possibility which corresponds to Gregory Belkin's method which is to apply half-wavelet transforms in certain regions of the matrix.  One section to be concerned about is the diagonal component.  In the case of the diagonal and average components, a full-wavelet transform may be necessary to cause the diagonal in those sections to diminish faster. 

\subsection {Evaluation Methodology}
The most obvious issue needed from any of %other
 these conditioning methods is for the end results to be correct.  The answers must numerically match the results obtained by the conventional implicit method.  To test this correctness, each solution conditioned by wavelets are checked against their implicit method counter-part.  

The second issue to test is efficiency.  There already exists the theoretical measure of efficiency.  However, an empirical study is also a result that is of interest.  The number of operations and the memory space consumed shall be considered measures of efficiency.  


%It is true that there are two forms that can be defined as the Recursive Multi-Resolution Wavelet Transform.  This is not referring to the different wavelet basis functions that can be used.  Rather, it is reference to how the multiple resolutions are applied to the matrix to obtain a similar matrix.   Recall from linear algebra that the solution to a matrix A is the same as a matrix B which is similar to A.  



%One form uses a quad-tree to ensure that recursion is applied through-out the matrix.  The other only applies it to the average term recursively.   The obvious difference here is that the second only considerer one branch of the tree to continue recursion on.  The first considers all branches.  The second offers the ability to arrive at a sparse matrix, and determine an optimal matrix.  

%Major Point: Use multi-resolution to eliminate the non-diagonal elements.  In a quad-tree multi-resolution wavelet transformed matrix, many diagonals are expected.  However the majority of the strength is expected to be in the main diagonal.  

%The primary objective of using these forms to solve PDE are to ensure that matrix is transformed into a similar matrix which is well conditioned and is sparse.  %issue that all related research regarding wavelets and partial differential equations is the need to produce a sparse and well-conditioned matrix for solving PDE via the implicit method.  
%In case of a quad-tree, all resolutions and components are available for consideration.  If it is the case that many diagonals are formed then this case must be examined.   One consequence of the primary goal is for a majority of the matrix strength is expected to be in the diagonal.   %Many diagonals are expected, and the majority of the matrix's strength is expected in the diagonal.  Also. many sections are expected to be zero.  


%Zero branches are expected to appear in the first three to seven levels (resolutions).  When this occurs, the result of the computation of the children can be assumed to be zero also.    

% Optimal Conditions: Zeroized Leaves, Concentration on the diagonal, and half transforms



%Concentration of non-zero leaves near the diagonal is the next step in optimization.  Careful measurement of energy of non-zero leaves in non-diagonal sections can be a problem.  One method for consideration is the arrangement of wavelet transform terms to ensure a diagonal matrix.  

%One feature of the 2-D wavelet transform, arrangement of the average, vertical, horizontal, and diagonal terms is relative, but not absolute.  Selection of the average term's corner is arbitrary.  The diagonal is always catty-corner to the average term.  The vertical and horizontal term positions are determined relative to the average term.  

%Zero branches are expected to appear in the first three to seven levels (resolution).  One optimal condition is highest level tree where a majority of the energy of the tree is concentrated in just a few leaves.   The other optimal condition is such that energy of the matrix is concentrated in the diagonal of the matrix.  

%The two optimal cases imply a cross reference condition such that both optimal conditions are satisfied.  In the quad-tree multi-resolution method, this is accomplished by using the tree to reference the boundaries of each individual component, while representing the data in the matrix.    A special value in the quad-tree's data structure should be energy.  When the leaves which correspond the diagonal region have the largest energy, and the sections away from that diagonal have energy sparsely concentrated in just a few leaves, then the matrix is optimized.  

%Possibility of half resolution?

%There is one possibility which corresponds to Gregory Belkin's method which is to apply half-wavelet transforms in certain regions of the matrix.  One section to be concerned about is the diagonal component.  In the case of the diagonal and average components, a full-wavelet transform may be necessary to cause the diagonal in those sections to diminish faster. 

%Application of recursive method in implicit solution.



%********************************************************************************

%\begin{thebibliography}{99}
%\bibitem {appliedmethods} Singiresu S. Rao \textsl{Applied Numerical Methods for Engineers and Scientists}  published Prentice Hall Upper Saddle River, NJ 07458 copyright  2002
%\bibitem {numrecipies} William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery 
%\textsl {Numerical Methods in C}
%Published by the Press Syndicate of the University of Cambridge The Pitt Building, Trumpington Street, Cambridge CB2 1RP%40 West 20th Street, New York, NY 10011-4211, USA
%Copyright Cambridge University Press 1988, 1992

%\bibitem {bvpbeylkin}  G. Beylkin \textsl{On wavelet-based algorithms for solving differential equations}

%\bibitem {amsbeylkin}  G. Beylkin \textsl{Wavelets and Fast Numerical Algorithms}

%\bibitem {fwtnal} G. Beylkin, R. Coifman, and V. Rokhlin \textsl {Fast Wavelet Transforms and Numerical Algorithms I, Article in Communications on pure and applied mathematics} copyright 1991 Wiley, New York

%\bibitem {mwabeylkin}  G. Beylkin, D. Gines and L. Vozovoi \textsl{Adaptive Solution of Partial Differential Equations in Multiwavelet Bases }

%\bibitem {PDEfSE} Stanly J. Farlow \textsl{Partial Differential Equations for Scientists and Engineers} copywrite 1993 Dover Publications, Inc Mineola, N.Y. 11501

%\bibitem {spiegel} Murray R. Spiegel, \textsl { Theory and Problems of Advanced Mathematics for Engineers and Scientist} copy-write 1996 McGraw-Hill 

%\bibitem {tools} Stephane Jaffard, Yves Meyer, and Robert D. Ryan \textsl {Wavelets: Tools for Science and Technology} copyright 2001 by the Society for Industrial and Applied Mathematics Philadelphia, PA 19104-2688

%\bibitem {victor} Mladen Victor Wickerhauser \textsl {Adapted Wavelet Analysis from Theory to Software} copyright 2001 by the Society for Industrial and Applied Mathematics Philadelphia, PA 19104-2688


%\end {thebibliography}


% \end{document}