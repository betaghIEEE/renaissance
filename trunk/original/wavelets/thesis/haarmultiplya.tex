%\documentclass[11pt]{article}
%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

%\textwidth = 6.5 in
%\textheight = 9 in
%\oddsidemargin = 0.0 in
%\evensidemargin = 0.0 in
%\topmargin = 0.0 in
%\headheight = 0.0 in
%\headsep = 0.0 in
%\parskip = 0.2in
%\parindent = 0.0in

%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}{Definition}

%\title{Brief Article}
%\author{The Author}
%\begin{document}
%\maketitle

In this chapter, the general concept of matrix multiplication via
wavelets is introduced. First, matrix multiplication and sparse matrix
multiplication itself is reviewed for completeness.  Then the $\psi^n$
transform with the Haar basis is shown to provide a precodnitioned
matrix which can be used to compute the matrix product. The correctness
of this method is demonstrated through a formal proof.

\section{Matrix Multiplication}

Matrix multiplication is one of the fundamental operations in linear algebra.  It is defined for two matrices $A$ and $B$, denoted $C = A\cdot B$.  Matrix multiply requires that the row length of A to be the same as the column length of B.  The value of element, $c_{i,j}$ is defined by
\[
c_{i,j} = \sum\limits_k a_{i,k} b_{k,j}.
\]
Matrix multiplication also obeys the following properties \cite{lipschutz}.
\begin{itemize}
\item Associative law:  $(AB)C = A(BC)$
\item Left Distributive Law $A (B+C) = AB + AC$
\item Right Distributive Law $(B+C)A = BA + CA$
\item Distribution of a constant
\end{itemize} 
For two generic $2 \times 2$ matrices, $A$ and $B$, the resulting products is
\begin{equation}\displaystyle
\label{eqn:conventional}
A \cdot B = 
\left(\begin{array}{cc}  a^1_1&  a^2_1 \\ a^1_2 &  a^2_2 \end{array}\right)
\left(\begin{array}{cc}  b^1_1&  b^2_1 \\ b^1_2 &  b^2_2 \end{array}\right) =
\left(\begin{array}{cc}  a^1_{1} b^1_{1} + a^2_{1} b^1_{2}&  a^1_{1}b^2_{1} + a^2_{1}  b^2_{2}    \\ a^1_{2} b^1_{1} + a^2_{2} b^1_{2} &  a^1_{2} b^2_{1} + a^2_{2} b^2_{2} 
\end{array}\right).
\end{equation}


There is a fast matrix multiplication which was devised in 1969 by  Strassen.  It achieves a computational complexity of $O(N^{2.807})$ for large $N$ representing one dimension of the matrix.  This works for square matrices.  There is a variation of the Strassen called the Winograd which obtains slightly better performance  \cite{lederman}.

Another view on matrix multiplication is sparse multiplication. The
whole point of sparse matrix multiplication is to take a matrix such
that the matrix composition is mostly zero, and exploit that
composition to reduce the number of multiplications required.  Such
multiplication schemes optimal limit for matrix multiplication is
$O(N^2)$. One method is included in the Sparse Matrix Multiplication
Package, called SYMBMM \cite {bank}. Another is contained in the Basic
Linear Algebra Subprogram for sparse matrix to dense matrix multiply.


%\bibitem {blackford} Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst, editors. \textsl {Templates for the Solution of Algebraic Eigenvalue Problems: a Practical Guide}. SIAM, Philadelphia, 2000.

%\bibitem {bank} Randolph E. Bank, Craig C. Douglas \textsl{Sparse Matrix Multiplication Package} April 23, 2001.

%\bibitem {lederman} Steven Huss-Lederman, Elaine M. Jacobson, J.R. Johnson, Anna Tsao, Thomas Turnbull \textsl {Strassen's Algorithm for Matrix Multiplication: Modeling, Analysis, and Implementation} November 15, 1996


\section {Wavelet Matrix Multiplication}
The point of this thesis is not show the efficiencies of these above algorithms.  Rather, it is to show a pre-conditioner using the wavelet transform that can be used in each of them.  %In the process examples of multi-resolution schemes that do not work shall be exposed.  The ones that work will be examined for fidelity and accuracy as elements are cast away.
Wavelet based matrix multiply is sound for the Haar Wavelet Transform and the $\psi^n$ expansion.   
%Wavelet based matrix multiplication is sound only as long as the wavelet operator is a linear operator.   Any optimizer applying the wavelet transform must ensure linearity is maintained.  
If the Haar Wavelet Transform produces a sparse and well-conditioned matrix, then the Haar Wavelet Transform proves itself as a useful preconditioner.  %sparseness and a well conditioned matrix result in the process, then the resulting matrix should be simpler to multiply.    
Here, the general concept of matrix multiplication via wavelets is introduced, and the linearity principle is shown.  
%One of t
The key point for wavelet matrix multiplication is the proof that $W(\alpha) \times W(\beta) = W(\alpha \times \beta)$  If this is the case, then it is obvious that $W(\alpha) \times W(\beta) = W(\Gamma) = W(\alpha \times \beta = \Gamma)$.  %So far the proof is still weak.  The reason is that an example proof is useful for proving something to not be the case, rather than being the case.   However, a simple example does show some intuitive steps that would be necessary for a proof.  



\subsection{A $2\times 2$ example}

The feasibility of multiplication in the wavelet domain is demonstrated directly using a $2 \times 2$ matrix. The coefficients of the matrices are multiplied both according to normal matrix multiplication and the modified wavelet multiplication operator. In the end the resulting coefficients are seen to be the same.

\subsubsection{Wavelet Transforms of the Matrices}

For a wavelet transform, the result on matrix $\alpha$ is
\begin{equation} \label{WA} \displaystyle
W(\alpha) = 
{1 \over 2} 
\left(\begin{array}{cc}  
\left(\alpha^1_1 + \alpha^1_2 + \alpha^2_1 + \alpha^2_2\right) & 
\left(\alpha^1_1 + \alpha^1_2 - \alpha^2_1 - \alpha^2_2\right)  \\ 
\left(\alpha^1_1 - \alpha^1_2 + \alpha^2_1 - \alpha^2_2\right) & 
\left(\alpha^1_1 - \alpha^1_2 - \alpha^2_1 + \alpha^2_2\right)   
\end{array}\right), 
\end{equation}
and for matrix $\beta$ it is
\begin{equation} \label{WB} \displaystyle
W(\beta) = {1 \over 2} 
\left(\begin{array}{cc}
\left(\beta^1_1 + \beta^1_2 + \beta^2_1 + \beta^2_2\right) &
\left(\beta^1_1 + \beta^1_2 - \beta^2_1 - \beta^2_2\right) \\ 
\left(\beta^1_1 - \beta^1_2 + \beta^2_1 - \beta^2_2\right) &  
\left(\beta^1_1 - \beta^1_2 - \beta^2_1 + \beta^2_2\right)
\end{array}\right).
\end{equation}

\subsubsection{Product of $A$ and $B$ in wavelet space}

The conventional product of $A$ and $B$ (equation \ref{eqn:conventional}) can be transformed into wavelet space. The wavelet transform of this matrix is represented by 
\[
W(\alpha \cdot \beta) =
{1 \over 2}
\left(
\begin{array}{cc}
\psi(A) & \psi(V) \\
\psi(H) & \psi(D)
\end{array}
\right)
\]
where 
\begin{eqnarray}
\label{eqn:abwavelet1}
\psi(A) &=& (\alpha^1_1 \beta^1_1 + \alpha^2_1 \beta^1_2 + \alpha^1_1\beta^2_1 + \alpha^2_1  \beta^2_2) + (\alpha^1_2 \beta^1_1 + \alpha^2_2 \beta^1_2 + \alpha^1_2 \beta^2_1 + \alpha^2_2 \beta^2_2)\\
\label{eqn:abwavelet2}
\psi(V) &=&(\alpha^1_1 \beta^1_1 + \alpha^2_1 \beta^1_2  - \alpha^1_1\beta^2_1 - \alpha^2_1  \beta^2_2) +  (\alpha^1_2 \beta^1_1 + \alpha^2_2 \beta^1_2 - \alpha^1_2 \beta^2_1 - \alpha^2_2 \beta^2_2 ) \\
\label{eqn:abwavelet3}
\psi(H) &=& (\alpha^1_1 \beta^1_1 + \alpha^2_1 \beta^1_2 + \alpha^1_1\beta^2_1 + \alpha^2_1  \beta^2_2) - (\alpha^1_2 \beta^1_1 + \alpha^2_2 \beta^1_2 + \alpha^1_2 \beta^2_1 + \alpha^2_2 \beta^2_2)\\
\label{eqn:abwavelet4}
\psi(D) &=& (\alpha^1_1 \beta^1_1 + \alpha^2_1 \beta^1_2  - \alpha^1_1\beta^2_1 - \alpha^2_1  \beta^2_2) - (\alpha^1_2 \beta^1_1 + \alpha^2_2 \beta^1_2 - \alpha^1_2 \beta^2_1 - \alpha^2_2 \beta^2_2 ).
\end{eqnarray}

\subsubsection{The product of the waveletized matrices}

Straightforward multiplication of $W(A) \cdot W(B)$ represented by equations \ref{WA} and \ref{WB} works out as follows:
\[
W(\alpha) \cdot W(\beta) = 
{1 \over 4} 
\left(
\begin{array}{cc}
W_A & W_V \\
W_H & W_D
\end{array}
\right)
\]
where
\begin{eqnarray*}
W_A &=& (\alpha^1_1 + \alpha^1_2 + \alpha^2_1 + \alpha^2_2)( \beta^1_1 + \beta^1_2 + \beta^2_1 + \beta^2_2) + (\alpha^1_1 +\alpha^1_2 -\alpha^2_1 -\alpha^2_2)(\beta^1_1 - \beta^1_2 + \beta^2_1 - \beta^2_2) \\
W_V &=& (\alpha^1_1 +\alpha^1_2 +\alpha^2_1 +\alpha^2_2) ( \beta^1_1 + \beta^1_2 - \beta^2_1 - \beta^2_2) + (\alpha^1_1 +\alpha^1_2 -\alpha^2_1 -\alpha^2_2) ( \beta^1_1 - \beta^1_2 - \beta^2_1 + \beta^2_2)  \\
W_H &=&  (\alpha^1_1 -\alpha^1_2 +\alpha^2_1 -\alpha^2_2)(\beta^1_1 + \beta^1_2 + \beta^2_1 + \beta^2_2) +  (\alpha^1_1 -\alpha^1_2 -\alpha^2_1 +\alpha^2_2 ) (\beta^1_1 - \beta^1_2 + \beta^2_1 - \beta^2_2) \\
W_D &=& (\alpha^1_1 -\alpha^1_2 +\alpha^2_1 -\alpha^2_2) (\beta^1_1 + \beta^1_2 - \beta^2_1 - \beta^2_2)+(\alpha^1_1 -\alpha^1_2 -\alpha^2_1 +\alpha^2_2 )(\beta^1_1 - \beta^1_2 - \beta^2_1 + \beta^2_2)
\end{eqnarray*}
which simplifies to
\begin{eqnarray*}
W_A &=&\alpha^1_1 \beta^1_1 +\alpha^1_2 \beta^1_1 +\alpha^2_1 \beta^1_2 +\alpha^2_2 \beta^1_2 +\alpha^1_1 \beta^2_1 +\alpha^1_2 \beta^2_1 +\alpha^2_1 \beta^2_2 +\alpha^2_2 \beta^2_2 \\
W_V &=& \alpha^1_1 \beta^1_1 +\alpha^1_2 \beta^1_1 +\alpha^2_1 \beta^1_2 +\alpha^2_2 \beta^1_2 -\alpha^1_1 \beta^2_1 -\alpha^1_2 \beta^2_1 -\alpha^2_1 \beta^2_2 -\alpha^2_2 \beta^2_2 \\
W_H &=&\alpha^1_1 \beta^1_1 -\alpha^1_2 \beta^1_1 +\alpha^2_1 \beta^1_2 -\alpha^2_2 \beta^1_2 +\alpha^1_1 \beta^2_1 -\alpha^1_2 \beta^2_1 +\alpha^2_1 \beta^2_2 -\alpha^2_2 \beta^2_2 \\
W_D &=&\alpha^1_1 \beta^1_1 -\alpha^1_2 \beta^1_1 +\alpha^2_1 \beta^1_2 -\alpha^2_2 \beta^1_2 -\alpha^1_1 \beta^2_1 +\alpha^1_2 \beta^2_1 -\alpha^2_1 \beta^2_2 +\alpha^2_2 \beta^2_2 
\end{eqnarray*}
These can then be compared to the coefficients of $W(A \cdot B)$ in equations \ref{eqn:abwavelet1}-\ref{eqn:abwavelet2} and seen to be identical. This asserts that $W(A) \cdot W(B) = W(A \cdot B)$ in the case of $2 \times 2$ matrices.

\subsection{Haar Wavelet Multiplication}

Given that the multiplication of the transformed coefficients is the
same as the transformation of the multiplied coefficients, an
algorithm can be developed to take advantage of this. The Haar Wavelet
Multiplication Algorithm is presented in Algorithm
\ref{alg:haarwaveletmultiply}. This produces a correct solution. 

\begin{algorithm}
\caption{ Haar Wavelet Multiplication}
\label{alg:haarwaveletmultiply}
\begin{algorithmic}
\REQUIRE Matrices $A$ and $B$
\STATE $\hat A \leftarrow \psi^n(A)$
\STATE $\hat B \leftarrow \psi^n(B)$
\STATE $\hat C \leftarrow \hat A \cdot \hat B$
\STATE $C \leftarrow \psi^{-n}\left(\hat C\right)$
\end{algorithmic}
\end{algorithm}

However, the preconditioned matrix has not been sparsified. In order to
sparsify the matrix, it is thresholded. Then, only nonzero elements are
retained and sparse matrix matrix multiplication can be employed on the
previously dense matrix. This is presented in Algorithm \ref{alg:sparsehaarmultiply}.


\begin{algorithm}
\caption{ Sparse Haar Wavelet Multiplication}
\label{alg:sparsehaarmultiply}
\begin{algorithmic}
\REQUIRE Matrices $A$ and $B$
\STATE $\hat A \leftarrow \psi^n(A)$
\STATE $\hat B \leftarrow \psi^n(B)$
\STATE $l_A \leftarrow$ sparsify $\left(\hat A\right)$
\STATE $l_B \leftarrow$ sparsify $\left(\hat B\right)$
\STATE $l_C \leftarrow$ sparse multiply $\left(l_A, l_B \right)$
\STATE $\hat C \leftarrow$ densify $\left(\hat C \right)$
\STATE $C \leftarrow \psi^{-n}\left(\hat C\right)$
\end{algorithmic}
\end{algorithm}
	

\section{Formal Proof of the Haar Wavelet Multiplication}

The formal proof of the correctness of the haar wavelet multiplication is presented below.


\subsection {Haar Wavelets and Vector Inner Products}
One of the other crucial keys for the Haar Wavelet Transform Matrix Multiply to work is vector inner product.  The issue is whether or not 
\begin{equation}\langle f', g' \rangle = \langle f,g \rangle \end{equation}
is true for vectors $f$ and $g$ which both of length $p$, and the wavelet transformed versions of $f$ and $g$ denoted $f'$ and $g'$ respectively.
To show this, $\langle f',g' \rangle$ is expanded algebraically to establish its identity.  
\begin{equation}
\label {vectinwt}\langle f',g' \rangle = \sum \limits_{k=0} ^{p/2 -1}(\frac{f_{2k} +f_{2k+1}}{\sqrt{2}} \cdot \frac{g_{2k} + g_{2k+1}}{\sqrt{2}}) + \sum \limits_{k=0} ^{p/2 -1}(\frac{f_{2k} - f_{2k+1}}{\sqrt{2}} \cdot \frac{g_{2k} - g_{2k+1}}{\sqrt{2}})\end{equation}
When equation \ref{vectinwt} is expanded further, the terms that emerge expose the identity of the inner product.  
\begin{eqnarray}\label {vectinwte}
\langle f',g' \rangle = \frac{1}{2}  \sum \limits_{k=0} ^{p/2 -1}
( f_{2k} g_{2k} + f_{2k+1}g_{2k} + f_{2k}g_{2k+1} + f_{2k+1}g_{2k+1}  \\ \nonumber+  f_{2k} g_{2k} - f_{2k+1}g_{2k} - f_{2k}g_{2k+1} + f_{2k+1}g_{2k+1} \end{eqnarray}
When simplified the $f_{2k}g_{2k+1}$ and $f_{2k+1}g_{2k}$ terms cancel.  The $f_{2k}g_{2k}$ and $f_{2k+1}g_{2k+1}$ terms combine to yield equation \ref{vectinwtsimple} and further simplifies to equation {vectinwtsimple2}.
\begin{equation}\label {vectinwtsimple}
\langle f',g' \rangle = \frac{1}{2}  \sum \limits_{k=0} ^{p/2 -1} (f_{2k}g_{2k} + f_{2k+1}g_{2k+1})\end{equation}
\begin{equation}\label {vectinwtsimple2}
\langle f',g' \rangle = \frac{1}{2}  \sum \limits_{k=0} ^{p-1} (f_{k}g_{k} ) = \langle f,g \rangle\end{equation}

\subsection{Matrix Multiply with the Haar Transform: Formal Proof}
\textbf{Given} two arbitary matrices $A$ of size $m\times p$, $B$ of size $p\times n$, the Haar Wavelet Pair, and the Haar Wavelet Transform.  The wavelet pair to be used here is 
\[ \psi_H (x)= \sqrt{\frac{1}{2}} \left\{\begin{array}{cc}1 & x=0 \\-1 & x=1 \\0 & otherwise\end{array}\right.\]
\[\phi_H (x) = \sqrt{\frac{1}{2}} \left\{\begin{array}{cc}1 & x=0\  {and}\ x=1 \\0 & otherwise\end{array}\right 
.. \]
%Vectors: f, g
%N $\times$ N matrices: A, B
%

Also, this notation is used for this proof.
\begin{itemize}\item $\psi_{1R}(S)$ is the row transform of matrix $S$.\item $\psi_{1C}(S)$ is the column transform of matrix $S$.
\item $\psi {S}$ is the 2-D wavelet transform of matrix $S$.
\item $\langle f,g \rangle = \langle \psi_1(f) , \psi_1(g) \rangle $
\item $A' = \psi(A)$
\item $B' = \psi(B)$\item $A^R =\psi_{1R}(A) $
\item $B^C =\psi_{1C}(B) $ \item $A^R_{ri}$ is the $i$th row vector of the row transform of $A$.  This is the same as saying $A_{ri} = A(i,:)$ \item $A^R_{ri+1}$ is the $i+1$th row vector  of the row transform of $A$. This is the same as saying $A_{ri+1} = A(i+1,:)$ 
\item $B^C_{cj}$ is the $j$th  column vector  of the column transform of $B$.  This is the same as saying $B_{cj} = B(:,j)$ 
\item $B^C_{cj+1}$ is the $j+1$th column vector of the column transform of $B$.  This is the same as saying $B_{cj+1} = B(:,j+1)$ \end{itemize}

\textbf{Required} show that 
\[ \psi (A) \psi (B) = \psi (A B) \]
is a true statement. %One fact that may also be useful is: \[ \left\leftangle \psi_1(f), \psi_1(g) \right\rightangle \ \equiv \left\leftangle f, g \right\rightangle \]


\textbf{Proof}

There are two formulae required for transforming either A or B into the wavelet domain.
\begin{equation}
\label{wavedefc4A}
 \psi (A) = \psi _{1R} ( \psi_{1C} (A)) = \psi _{1C} ( \psi_{1R} (A))
\end{equation}
\begin{equation}
\label{wavedefc4B}
\psi (B) = \psi _{1R} ( \psi_{1C} (B)) = \psi _{1C} ( \psi_{1R} (B))    
\end{equation}

Thus, this is simply a re-write of the definition of the wavelet transform by the CWT.   Next,  define a matrix $\Gamma'$  so that
\[\Gamma' = \psi (A) \psi (B). \]  From, the definition $\Gamma'$, a series of re-writes and the properties of the Haar Wavelet Transform can expose the solution for every element of $\Gamma'$ and $\Gamma'$.  In general, the elements of $\Gamma'$ defined as follows:
\begin{equation}
\label{wavecrossA}
\Gamma'_{i,j} =   \left\langle \psi_{1C} (\psi_{1R} (A)) _{ri} , \psi_{1R} (\psi_{1C} (B)) _{ci}  \right\rangle
\end{equation}
\begin{equation}
\label{wavecrossB}
\Gamma'_{i,j} =   \left\langle \psi_{1R} (\psi_{1C} (A)) _{ri} , \psi_{1C} (\psi_{1R} (B)) _{ci}  \right\rangle
\end{equation}

In this case, each element of $\Gamma'$ is defined as nothing more than the inner product of row $i$ of $\psi(A)$ and column $j$ of $\psi(B)$.  This is in agreement with standard matrix multiplication.  %In this case, discrete (vector) inner product is the inner product form being used, and is defined for vectors $f$ and $g$ of length $n$
%\begin{equation}
%\label{innerproduct}
%\langle f, g\rangle =  \sum_{k=1} ^n f_k \cdot g_k
%\end{equation}
%
%Another re-write of $\Gamma'$ now is in order.  This re-write uses the definition of inner product  along with results of the Haar Transform.

%\[  \left\langle A'_{ri} , B'_{cj}  \right\rangle =  \sum\limits_{k=0}^{row/2-1}\frac{1}{2}%
%((A^R_{ri})_{k}+(A^R_{ri})_{k+1})((B^C_{cj})_{k}+(B^C_{cj})_{k+1}))  + \sum\limits_{k=0}^{row/2-1}\frac{1}{2}%
%((A^R_{ri})_{k}-(A^R_{ri})_{k+1})((B^C_{cj})_{k}-(B^C_{cj})_{k+1})) \]
%The re-write of $\Gamma'$ reduces the operation to a one-dimensional wavelet transform.  Now four cases are established by the follow definitions for the Haar Wavelet Transform (in one-dimension) for either column transforms or row transforms:
Another identity that is important what the wavelet pair for transform will do in either a row or column transform.  
\[  \psi _{1C} (A) = \left\{ 
\begin{array}{ll}
\frac{A_{i,j}+A_{i,j+1}}{\sqrt{2}} & j< \frac{n}{2} \\ 
\frac{A_{i,j}-A_{i,j+1}}{\sqrt{2}} & j\geq  \frac{n}{2}%
\end{array}
\right.   \]
 \[  \psi _{1R} (A) = \left\{ 
\begin{array}{ll}
\frac{A_{i,j}+A_{i+1,j}}{\sqrt{2}} & i< \frac{m}{2} \\ 
\frac{A_{i,j}-A_{i+1,j}}{\sqrt{2}} & i\geq  \frac{m}{2}%
\end{array}
\right. 
 \]
 %insert table for cases
 
In this case, there are four cases to show for:
\begin{enumerate}\item $i < \frac{m}{2}$ and $j <  \frac{n}{2}$
\item $i \geq \frac{m}{2}$ and $j <  \frac{n}{2}$
\item $i < \frac{m}{2}$ and $j \geq  \frac{n}{2}$
\item $i \geq \frac{m}{2}$ and $j \geq  \frac{n}{2}$\end{enumerate}

 \textbf{Case 1} $i < \frac{m}{2} $ and $j < \frac{n}{2}$.
The base equation $\Gamma'_{i,j}$ in this cases is as follows:
\begin{equation}
\label{case1inner}
 \Gamma'_{ij} = \left\langle \frac {A^R_{ri} + A^R_{ri+1}}{\sqrt{2}} , \frac {B^C_{cj} + B^C_{cj + 1}}{\sqrt{2}} \right\rangle
\end{equation}

%This definition is valid of $i\in [0,\frac{r_l}{2} -1]$ and $j\in [0, 
This re-write must be expanded to expose an equality.  This expansion is valid for vectors sums within inner products.   Thus $\Gamma'_{i,j}$ in this case is expanded to:
\[ \Gamma'_{i,j} = \frac{1}{2} ( \left\langle A^R_{ri}, B^C_{cj}   \right\rangle +   \left\langle A^R_{ri+1}, B^C_{cj}   \right\rangle +  \left\langle A^R_{ri}, B^C_{cj + 1}   \right\rangle +  \left\langle A^R_{ri+1}, B^C_{cj+1}   \right\rangle    ). \]

Next, $\Gamma'_{i,j}$ must be compared to $\psi(\Gamma)_{i,j}$ where
\[ \psi (A B)  = \psi (\Gamma)    \]
By definition, every element in $C$ is defined:
\[ \Gamma_{i,j} =   \left\langle A_{ri} , B_{cj}  \right\rangle \]
The next step is to show what every element within this case is for $\psi(\Gamma)$.  This is done by analyzing what the column transform will do at $C_{i,j}$.  To do this, the column transform is applied to both column $j$ and $j+1$ for columns between $[0,\frac{col}{2} -1]$.  The effects of these column transforms are defined in equations \ref{rowwtC1c1} and \ref{rowwtC2c1}.
\begin{equation}\displaystyle\label {rowwtC1c1}
 \psi_{1C} (\Gamma)_{i,j} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj}  \right\rangle + \left\langle A_{ri +1} , B_{cj}  \right\rangle \end{equation}
\begin{equation}\label {rowwtC2c1}
\psi_{1C} (\Gamma)_{i,j+1} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj+1}  \right\rangle + \left\langle A_{ri +1} , B_{cj+1}  \right\rangle \\end{equation}

The result of the row transform on $\psi_{1C}(\Gamma)$ in this case is a sum of $\psi_{1C}(\Gamma)_{i,j}$ and $\psi_{1C}(\Gamma)_{i,j+1}$, and specified in equation \ref{rowtfCc1}.
\begin{equation}\label{rowtfCc1}
 \psi(\Gamma) = \frac{1}{\sqrt {2}} (\psi_{1C} (\Gamma)_{i,j} + \psi_{1C} (\Gamma)_{i,j+1}   )\end{equation}
 Expanded this equation is equation \ref{rowtfCc1e}.
\begin{equation}\label{rowtfCc1e}
 \psi(\Gamma) = \frac{1}{2} (  \left\langle A_{ri} , B_{cj}  \right\rangle + \left\langle A_{ri +1} , B_{cj}  \right\rangle + \left\langle A_{ri} , B_{cj+1}  \right\rangle + \left\langle A_{ri +1} , B_{cj+1}  \right\rangle   ) \end{equation}


\textbf{Case 2} $i \geq \frac{m}{2}$ and $j <  \frac{n}{2}$.
The base equation $\Gamma'_{i,j}$ in this cases is as follows:
\[ \Gamma'_{ij} = \left\langle \frac {A^R_{ri,j} - A^R_{ri+1}}{\sqrt{2}} , \frac {B^C_{cj} + B^C_{cj + 1}}{\sqrt{2}} \right\rangle  \]
The next step is to show what every element within this case is for $\psi(\Gamma)$.  
\[ \Gamma'_{i,j} = \frac{1}{2} ( \left\langle A^R_{ri}, B^C_{cj}   \right\rangle -   \left\langle A^R_{ri+1}, B^C_{cj}   \right\rangle +  \left\langle A^R_{ri}, B^C_{cj + 1}   \right\rangle -  \left\langle A^R_{ri+1}, B^C_{cj+1}   \right\rangle    ) \]
How does this compare with $\psi (\Gamma)$?
\[ \psi (A B)  = \psi (\Gamma)    \]
\[ (\Gamma) = \psi  \left\langle A_{ri} , B_{cj}  \right\rangle \]
This equation expands with wavelet operator to:
\[ \psi_{1C} (\Gamma)_{i,j} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj}  \right\rangle - \left\langle A_{ri +1} , B_{cj}  \right\rangle   \]
\[ \psi_{1C} (\Gamma)_{i,j+1} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj+1}  \right\rangle - \left\langle A_{ri +1} , B_{cj+1}  \right\rangle \  \]

The result of the row transform on $\psi_{1C}(\Gamma)$ in this case is a sum of $\psi_{1C}(\Gamma)_{i,j}$ and 
\[ \psi(\Gamma) = \frac{1}{\sqrt {2}} (\psi_{1C} (\Gamma)_{i,j} + \psi_{1C} (\Gamma)_{i,j+1}   ) \]

 Expanded this equation is equation \ref{rowtfCc1e}.
\[ \psi(\Gamma) = \frac{1}{2} (  \left\langle A_{ri} , B_{cj}  \right\rangle - \left\langle A_{ri +1} , B_{cj}  \right\rangle + \left\langle A_{ri} , B_{cj+1}  \right\rangle - \left\langle A_{ri +1} , B_{cj+1}  \right\rangle   ) \]

\textbf {Case 3} $i < \frac {m}{2}$ and $j \ge \frac{n}{2}$.
Like, before the base case for $\Gamma'_{i,j}$
\[ \Gamma'_{ij} = \left\langle \frac {A^R_{ri,j} + A^R_{ri+1}}{\sqrt{2}} , \frac {B^C_{cj} - B^C_{cj + 1}}{\sqrt{2}} \right\rangle  \]
Again, the vector sums are expanded within their inner products.  
\[ \Gamma'_{i,j} = \frac{1}{2} ( \left\langle A^R_{ri}, B^C_{cj}   \right\rangle +   \left\langle A^R_{ri+1}, B^C_{cj}   \right\rangle -  \left\langle A^R_{ri}, B^C_{cj + 1}   \right\rangle -  \left\langle A^R_{ri+1}, B^C_{cj+1}   \right\rangle    ) \]
Next, $\Gamma'_{i,j}$ is compared with $\psi(\Gamma)$
\[ \psi (A B)  = \psi (\Gamma)  .  \]
Next, the column transform expansion is conducted:
\[ (\Gamma) = \psi  \left\langle A_{ri} , B_{cj}  \right\rangle \]
\[ \psi_{1C} (\Gamma)_{i,j} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj}  \right\rangle + \left\langle A_{ri +1} , B_{cj}  \right\rangle   \]
\[ \psi_{1C} (\Gamma)_{i,j+1} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj+1}  \right\rangle + \left\langle A_{ri +1} , B_{cj+1}  \right\rangle \  \]
Now the column transform results are combined, by subtraction in this case:
\[ \psi(\Gamma) = \frac{1}{\sqrt {2}} (\psi_{1C} (\Gamma)_{i,j} - \psi_{1C} (\Gamma)_{i,j+1}   ) \]
Lastly, the expanded form is expanded.
\[ \psi(\Gamma) = \frac{1}{2} (  \left\langle A_{ri} , B_{cj}  \right\rangle + \left\langle A_{ri +1} , B_{cj}  \right\rangle - \left\langle A_{ri} , B_{cj+1}  \right\rangle - \left\langle A_{ri +1} , B_{cj+1}  \right\rangle   ) \]

 \textbf{Case 4} $i \ge \frac {m}{2}$ and $j \ge \frac{n}{2}$.
Like, before the base case for $\Gamma'_{i,j}$ 
\[ \Gamma'_{ij} = \left\langle \frac {A^R_{ri,j} - A^R_{ri+1}}{\sqrt{2}} , \frac {B^C_{cj} - B^C_{cj + 1}}{\sqrt{2}} \right\rangle  \]
Again, the vector sums are expanded within their inner products. 
\[ \Gamma'_{i,j} = \frac{1}{2} ( \left\langle A^R_{ri}, B^C_{cj}   \right\rangle -   \left\langle A^R_{ri+1}, B^C_{cj}   \right\rangle -  \left\langle A^R_{ri}, B^C_{cj + 1}   \right\rangle +  \left\langle A^R_{ri+1}, B^C_{cj+1}   \right\rangle    ) \]

Next, $\Gamma'_{i,j}$ is compared with $\psi(\Gamma)$
\[ \psi (A B)  = \psi (\Gamma)    \]
\[ (\Gamma) = \psi  \left\langle A_{ri} , B_{cj}  \right\rangle . \]
Once the column wavelet transform expands $\Gamma$ to
\[ \psi_{1C} (\Gamma)_{i,j} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj}  \right\rangle - \left\langle A_{ri +1} , B_{cj}  \right\rangle   \]
\[ \psi_{1C} (\Gamma)_{i,j+1} = \frac{1}{\sqrt{2}} \left\langle A_{ri} , B_{cj+1}  \right\rangle - \left\langle A_{ri +1} , B_{cj+1}  \right\rangle. \]

Now the column transform results are combined, by subtraction in this case:
\[ \psi(\Gamma) = \frac{1}{\sqrt {2}} (\psi_{1C} (\Gamma)_{i,j} - \psi_{1C} (\Gamma)_{i,j+1}   ). \]

Lastly, the expanded form is expanded.
\[ \psi(\Gamma) = \frac{1}{2} (  \left\langle A_{ri} , B_{cj}  \right\rangle - \left\langle A_{ri +1} , B_{cj}  \right\rangle - \left\langle A_{ri} , B_{cj+1}  \right\rangle + \left\langle A_{ri +1} , B_{cj+1}  \right\rangle   ) \]

Since the inner product of two vectors $f,g$ is the same as their wavelet transformed vectors $f',g'$, then all 4 cases produce a case where $\Gamma'_{i,j} = (\psi (\Gamma))_{i,j} \ \forall (i,j)$.  Thus for the Haar Wavelet Transform (discrete mother basis), 
\[ \psi (A) \cdot \psi(B) = \psi (A\cdot B) \].

\section {Multi Resolution Expansion Proof}
In a previous section, wavelet based matrix multiplication was proven such that
\[\psi(A) \cdot \psi(B) = \psi (A\cdot B) \]
In this section the question of whether or not there is MRE form which is sound for matrix multiplication.  There are few facts that are relevant and were made obvious in the empirical analysis in the results %section of this 
chapter.  
\begin{itemize}\item $\psi_{WPx} (A) \not= \psi^x(A) $  where $\psi_{WPx} (A) $ is the $x$ resolution of wavelet transform packets (full decomposition) except for $x=1$.
\item $\psi_{Wx} (A) \not= \psi^x(A) $  where $\psi_{Wx} (A) $ is the $x$ resolution of wavelet transform pyramids except for $x=1$.\end{itemize}

The next hypothesis to be answered is can the wavelet transform be applied more than once to condition a matrix for matrix multiplication?  This question is answered by the following lemma.

\[ \psi ^2 (A) \cdot \psi^2 (B)  = \psi^2(A\cdot B) \]
Proof: \\
%\begin{enumerate}%\item %\end{enumerate}
The theorem \[ \psi(A) \cdot \psi(B) = \psi (A\cdot B) \] is proven as fact.
\[ \psi^2(A) = \psi(\psi (A))\]
\[ \psi^2 (A) =\cdot \psi^2 (B) = \psi(\psi(A)) \cdot \psi (\psi (B))  \]
\[ \psi(\psi(A)) \cdot \psi (\psi (B)) = \psi (\psi(A) \cdot \psi(B) ) \]
\[ \psi (\psi(A) \cdot \psi(B) ) = \psi (\psi(A \cdot (B))  \]
\[ \psi (\psi(A \cdot (B)) = \psi^2(A \cdot B) \]
Therefore: 
\[ \psi ^2 (A) \cdot \psi^2 (B)= \psi^2(A \cdot B) \]


% \end{document}
